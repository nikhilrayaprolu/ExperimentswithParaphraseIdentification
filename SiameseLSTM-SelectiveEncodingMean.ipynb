{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "def test(model, args, data, mode='test'):\n",
    "    if mode == 'dev':\n",
    "        iterator = iter(data.dev_iter)\n",
    "    else:\n",
    "        iterator = iter(data.test_iter)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    acc, loss, size = 0, 0, 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda()\n",
    "                char_h = char_h.cuda()\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = model(**kwargs)\n",
    "        pred = pred.view(-1,2)\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "\n",
    "        _, pred = pred.max(dim=1)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "\n",
    "    acc /= size\n",
    "    acc = acc.cpu().data[0]\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def load_model(args, data):\n",
    "    model = BIMPM(args, data)\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "\n",
    "    if args.gpu > -1:\n",
    "        model.cuda()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    Examples::\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_out = nn.Linear(dim*2, dim)\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, output, context):\n",
    "        batch_size = output.size(0)\n",
    "        hidden_size = output.size(2)\n",
    "        input_size = context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, input_size)).view(batch_size, -1, input_size)\n",
    "\n",
    "        # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        mix = torch.bmm(attn, context)\n",
    "\n",
    "        # concat -> (batch, out_len, 2*dim)\n",
    "        combined = torch.cat((mix, output), dim=2)\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = F.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return output, attn\n",
    "    \n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, args, data, use_attention = False):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n",
    "        self.l = self.args.num_perspective\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n",
    "\n",
    "        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        # no fine-tuning for word vectors\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        self.trainingtype = args.training\n",
    "        self.use_attention = use_attention\n",
    "        if self.use_attention:\n",
    "            self.attention = Attention(self.args.hidden_size*2)\n",
    "        \n",
    "        self.char_LSTM = nn.LSTM(\n",
    "            input_size=self.args.char_dim,\n",
    "            hidden_size=self.args.char_hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        self.context_LSTM = nn.LSTM(\n",
    "            input_size=self.d,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.aggregation_LSTM = nn.LSTM(\n",
    "            input_size=self.args.hidden_size*2,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.Ws = nn.Parameter(torch.rand(self.args.hidden_size*2,self.args.hidden_size*2))\n",
    "        self.Us = nn.Parameter(torch.rand(self.args.hidden_size*2,self.args.hidden_size*2))\n",
    "        self.bs = nn.Parameter(torch.rand(self.args.hidden_size*2))\n",
    "        \n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 4, self.args.hidden_size * 2)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 2, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Word Representation Layer -----\n",
    "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
    "        # zero vectors for padding\n",
    "        self.char_emb.weight.data[0].fill_(0)\n",
    "\n",
    "        # <unk> vectors is randomly initialized\n",
    "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
    "\n",
    "        nn.init.kaiming_normal(self.char_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.char_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0_reverse, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0_reverse, val=0)\n",
    "\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        p = self.word_emb(kwargs['p'])\n",
    "        h = self.word_emb(kwargs['h'])\n",
    "\n",
    "        if self.args.use_char_emb:\n",
    "            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n",
    "            seq_len_p = kwargs['char_p'].size(1)\n",
    "            seq_len_h = kwargs['char_h'].size(1)\n",
    "\n",
    "            char_p = kwargs['char_p'].view(-1, self.args.max_word_len)\n",
    "            char_h = kwargs['char_h'].view(-1, self.args.max_word_len)\n",
    "\n",
    "            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n",
    "            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n",
    "            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n",
    "\n",
    "            # (batch, seq_len, char_hidden_size)\n",
    "            char_p = char_p.view(-1, seq_len_p, self.args.char_hidden_size)\n",
    "            char_h = char_h.view(-1, seq_len_h, self.args.char_hidden_size)\n",
    "\n",
    "            # (batch, seq_len, word_dim + char_hidden_size)\n",
    "            p = torch.cat([p, char_p], dim=-1)\n",
    "            h = torch.cat([h, char_h], dim=-1)\n",
    "\n",
    "        p = self.dropout(p)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        # (batch, seq_len, hidden_size * 2)\n",
    "        #self.context_LSTM.flatten_parameters()\n",
    "        con_p, _ = self.context_LSTM(p)\n",
    "        con_h, _ = self.context_LSTM(h)\n",
    "        \n",
    "\n",
    "        #print(con_p.shape)\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "        \n",
    "        \n",
    "        p_key = torch.cat([con_p_fw[:,-1,:],con_p_bw[:,0,:]], dim=-1)\n",
    "        h_key = torch.cat([con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        #print(self.Ws.shape, con_p.shape, self.Us.shape, p_key.shape, torch.matmul(con_p,self.Ws).shape , torch.matmul(p_key,self.Us).view(-1,1,self.args.hidden_size*2).expand(con_p.shape[0],con_p.shape[1],con_p.shape[2]).shape , self.bs.shape)\n",
    "        sGatep = F.sigmoid(torch.matmul(con_p,self.Ws) + torch.matmul(h_key,self.Us).view(-1,1,self.args.hidden_size*2).expand(con_p.shape[0],con_p.shape[1],con_p.shape[2]) + self.bs)\n",
    "        sGateh = F.sigmoid(torch.matmul(con_h,self.Ws) + torch.matmul(p_key,self.Us).view(-1,1,self.args.hidden_size*2).expand(con_h.shape[0],con_h.shape[1],con_h.shape[2]) + self.bs)\n",
    "        con_p = sGatep * con_p\n",
    "        con_h = sGateh * con_h\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "        con_p_mean = torch.mean(con_p, 1, True)\n",
    "        con_h_mean = torch.mean(con_h, 1, True)\n",
    "        #print(p_enc_output_mean.shape, h_enc_output_mean.shape)\n",
    "        x = torch.cat(\n",
    "            [con_p_mean,con_h_mean], dim=-1)\n",
    "        \n",
    "        #p_key = torch.cat([con_p_fw[:,-1,:],con_p_bw[:,0,:]], dim=-1)\n",
    "        #h_key = torch.cat([con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "\n",
    "        #print(h_key.shape, con_p.shape, con_h_fw.shape,con_h_fw[:,-1,:].shape, )\n",
    "        #p_attn_output, attn_p = self.attention(h_key.view(-1,1,h_key.shape[1]), con_p)\n",
    "        \n",
    "        #h_attn_output, attn_h = self.attention(p_key.view(-1,1,p_key.shape[1]), con_h)\n",
    "        #print(p_attn_output.shape)\n",
    "        x = torch.cat([con_p_mean, con_h_mean], dim=-1)\n",
    "        \n",
    "        #print(con_p_fw[:,-1,:].shape)\n",
    "        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n",
    "        #x = torch.cat(\n",
    "        #    [con_p_fw[:,-1,:],con_p_bw[:,0,:],con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading Quora data...\n",
      "training start!\n",
      "('epoch:', '1')\n",
      "train loss: 290.245848909 / dev loss: 49.9691486657/ test loss:49.978054136 / dev acc:0.660299956799test acc:0.660499989986\n",
      "train loss: 269.034368232 / dev loss: 45.4503088593/ test loss:45.5297141671 / dev acc:0.703099966049test acc:0.696699976921\n",
      "train loss: 259.997570246 / dev loss: 41.835370332/ test loss:42.0734361783 / dev acc:0.725899994373test acc:0.718499958515\n",
      "train loss: 251.199654534 / dev loss: 42.2082631886/ test loss:42.6529608816 / dev acc:0.731000006199test acc:0.719199955463\n",
      "train loss: 240.734072134 / dev loss: 40.1081396937/ test loss:40.411663115 / dev acc:0.740599989891test acc:0.735599994659\n",
      "train loss: 233.664630499 / dev loss: 39.4517196119/ test loss:39.7849212885 / dev acc:0.752999961376test acc:0.746899962425\n",
      "('epoch:', '2')\n",
      "train loss: 223.945155464 / dev loss: 38.1399104297/ test loss:37.8885010853 / dev acc:0.75729995966test acc:0.755099952221\n",
      "train loss: 219.128108688 / dev loss: 36.7874051929/ test loss:36.2814732194 / dev acc:0.772699952126test acc:0.769800007343\n",
      "train loss: 215.821440212 / dev loss: 36.3963426948/ test loss:36.0403555557 / dev acc:0.77889996767test acc:0.776499986649\n",
      "train loss: 212.235470012 / dev loss: 35.3755982071/ test loss:35.2567461692 / dev acc:0.783899962902test acc:0.782199978828\n",
      "train loss: 210.224226326 / dev loss: 36.7553756684/ test loss:36.5542589687 / dev acc:0.777199983597test acc:0.772399961948\n",
      "train loss: 206.847472765 / dev loss: 34.9703861773/ test loss:34.7571014687 / dev acc:0.786499977112test acc:0.782000005245\n",
      "('epoch:', '3')\n",
      "train loss: 192.194985211 / dev loss: 33.6345863193/ test loss:33.2021764591 / dev acc:0.798399984837test acc:0.798799991608\n",
      "train loss: 194.48322513 / dev loss: 35.5602310598/ test loss:35.3180315197 / dev acc:0.78409999609test acc:0.779100000858\n",
      "train loss: 190.192622051 / dev loss: 34.6039167345/ test loss:33.9975546598 / dev acc:0.793399989605test acc:0.791799962521\n",
      "train loss: 190.893975064 / dev loss: 34.7070102394/ test loss:34.3564598784 / dev acc:0.796199977398test acc:0.793500006199\n",
      "train loss: 190.937402651 / dev loss: 32.2998042852/ test loss:32.25013531 / dev acc:0.807299971581test acc:0.804799973965\n",
      "train loss: 190.958501928 / dev loss: 32.7757624686/ test loss:32.347299993 / dev acc:0.807299971581test acc:0.806400001049\n",
      "('epoch:', '4')\n",
      "train loss: 173.468392313 / dev loss: 33.5025017411/ test loss:33.172500059 / dev acc:0.803399980068test acc:0.806999981403\n",
      "train loss: 176.343536884 / dev loss: 32.2552548498/ test loss:31.6576992236 / dev acc:0.810400009155test acc:0.817699968815\n",
      "train loss: 173.278443236 / dev loss: 32.098898828/ test loss:31.9868660867 / dev acc:0.814399957657test acc:0.81099998951\n",
      "train loss: 175.423671 / dev loss: 31.8291561306/ test loss:31.3191429935 / dev acc:0.814199984074test acc:0.816399991512\n",
      "train loss: 175.596327428 / dev loss: 31.9985235184/ test loss:31.3592398614 / dev acc:0.819499969482test acc:0.817399978638\n",
      "train loss: 175.301486 / dev loss: 32.0025111288/ test loss:31.9929489419 / dev acc:0.814300000668test acc:0.812900006771\n",
      "('epoch:', '5')\n",
      "train loss: 159.526331991 / dev loss: 33.8844589591/ test loss:33.598003678 / dev acc:0.815199971199test acc:0.814399957657\n",
      "train loss: 160.847612593 / dev loss: 33.9910026789/ test loss:33.3438731804 / dev acc:0.810699999332test acc:0.808699965477\n",
      "train loss: 162.504401952 / dev loss: 31.6183397323/ test loss:30.9335854799 / dev acc:0.821799993515test acc:0.824400007725\n",
      "train loss: 163.385397255 / dev loss: 32.6698569357/ test loss:32.070969969 / dev acc:0.812599956989test acc:0.810799956322\n",
      "train loss: 160.732009076 / dev loss: 30.65237315/ test loss:30.3763274103 / dev acc:0.828099966049test acc:0.820299983025\n",
      "train loss: 164.305850338 / dev loss: 32.2212511152/ test loss:31.5118807033 / dev acc:0.816199958324test acc:0.816199958324\n",
      "('epoch:', '6')\n",
      "train loss: 147.293301605 / dev loss: 32.5308629125/ test loss:31.4964193217 / dev acc:0.820599973202test acc:0.823300004005\n",
      "train loss: 149.294479057 / dev loss: 31.0745569468/ test loss:30.3572883345 / dev acc:0.826699972153test acc:0.82699996233\n",
      "train loss: 150.879181154 / dev loss: 32.1954598278/ test loss:31.2386511788 / dev acc:0.82539999485test acc:0.828299999237\n",
      "train loss: 150.936317176 / dev loss: 31.2586859912/ test loss:30.6458557099 / dev acc:0.823099970818test acc:0.823499977589\n",
      "train loss: 152.473146763 / dev loss: 31.7950303555/ test loss:31.1884600334 / dev acc:0.820699989796test acc:0.824499964714\n",
      "train loss: 153.727890663 / dev loss: 30.5301201791/ test loss:30.0083358884 / dev acc:0.833199977875test acc:0.827799975872\n",
      "('epoch:', '7')\n",
      "train loss: 134.946058754 / dev loss: 32.706222266/ test loss:31.2668471262 / dev acc:0.828700006008test acc:0.828799962997\n",
      "train loss: 139.539751627 / dev loss: 32.1327602267/ test loss:31.0724875629 / dev acc:0.822399973869test acc:0.825800001621\n",
      "train loss: 141.790606037 / dev loss: 31.3240597993/ test loss:30.6139498055 / dev acc:0.829400002956test acc:0.825999975204\n",
      "train loss: 143.460638821 / dev loss: 31.9664027542/ test loss:31.051299125 / dev acc:0.827499985695test acc:0.831200003624\n",
      "train loss: 142.760151401 / dev loss: 31.675715521/ test loss:30.8864800856 / dev acc:0.829499959946test acc:0.827299952507\n",
      "train loss: 144.863347985 / dev loss: 30.911906302/ test loss:29.9966588169 / dev acc:0.831299960613test acc:0.832399964333\n",
      "('epoch:', '8')\n",
      "train loss: 128.793737765 / dev loss: 33.123760134/ test loss:32.6718276925 / dev acc:0.826299965382test acc:0.822399973869\n",
      "train loss: 130.258405369 / dev loss: 33.028828159/ test loss:31.8588238955 / dev acc:0.828700006008test acc:0.825800001621\n",
      "train loss: 131.098278962 / dev loss: 33.2422468811/ test loss:32.0589151755 / dev acc:0.829499959946test acc:0.831399977207\n",
      "train loss: 140.215894779 / dev loss: 32.8290209025/ test loss:31.5718290508 / dev acc:0.826399981976test acc:0.827699959278\n",
      "train loss: 135.559658384 / dev loss: 32.3935505301/ test loss:30.8600750118 / dev acc:0.828799962997test acc:0.834299981594\n",
      "train loss: 136.496250644 / dev loss: 30.8408536613/ test loss:30.0656398237 / dev acc:0.831399977207test acc:0.828899979591\n",
      "('epoch:', '9')\n",
      "train loss: 120.981516216 / dev loss: 32.6255004853/ test loss:31.1413444057 / dev acc:0.831999957561test acc:0.833199977875\n",
      "train loss: 123.608738499 / dev loss: 34.5175360739/ test loss:32.6471333951 / dev acc:0.826699972153test acc:0.827999949455\n",
      "train loss: 125.106859898 / dev loss: 32.3071109951/ test loss:31.2581342608 / dev acc:0.829899966717test acc:0.832099974155\n",
      "train loss: 128.0926743 / dev loss: 32.7068391293/ test loss:31.3801092952 / dev acc:0.833899974823test acc:0.829799950123\n",
      "train loss: 128.362850353 / dev loss: 31.5607231408/ test loss:30.5220901668 / dev acc:0.839599967003test acc:0.832199990749\n",
      "train loss: 131.127718143 / dev loss: 32.2363354266/ test loss:31.0961664617 / dev acc:0.830999970436test acc:0.829499959946\n",
      "('epoch:', '10')\n",
      "train loss: 113.613248106 / dev loss: 33.7471062243/ test loss:33.0245025121 / dev acc:0.829099953175test acc:0.825800001621\n",
      "train loss: 119.421487942 / dev loss: 32.9662190825/ test loss:31.6198681556 / dev acc:0.832399964333test acc:0.833799958229\n",
      "train loss: 117.212921783 / dev loss: 34.4908544719/ test loss:33.067939952 / dev acc:0.833399951458test acc:0.832099974155\n",
      "train loss: 122.911740191 / dev loss: 32.122178182/ test loss:30.942898836 / dev acc:0.833999991417test acc:0.836399972439\n",
      "train loss: 124.508133333 / dev loss: 32.6575559676/ test loss:31.1891770512 / dev acc:0.840299963951test acc:0.838199973106\n",
      "train loss: 122.63654352 / dev loss: 31.401958093/ test loss:30.6229500733 / dev acc:0.834699988365test acc:0.836399972439\n",
      "('epoch:', '11')\n",
      "train loss: 109.331938528 / dev loss: 34.437030375/ test loss:33.2602763921 / dev acc:0.837300002575test acc:0.834999978542\n",
      "train loss: 112.428299781 / dev loss: 32.5682136714/ test loss:31.7799193785 / dev acc:0.838499963284test acc:0.833399951458\n",
      "train loss: 113.123962192 / dev loss: 34.4890060872/ test loss:33.7913106084 / dev acc:0.833899974823test acc:0.838099956512\n",
      "train loss: 114.923734348 / dev loss: 31.8963979632/ test loss:30.9843169302 / dev acc:0.833999991417test acc:0.834800004959\n",
      "train loss: 118.338327341 / dev loss: 33.2193784714/ test loss:32.2228226736 / dev acc:0.8382999897test acc:0.836399972439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 118.603808068 / dev loss: 31.6151800305/ test loss:30.9301172346 / dev acc:0.83679997921test acc:0.835199952126\n",
      "('epoch:', '12')\n",
      "train loss: 105.570005868 / dev loss: 33.889584139/ test loss:32.9811750166 / dev acc:0.833899974823test acc:0.833399951458\n",
      "train loss: 105.337955436 / dev loss: 34.0648011118/ test loss:32.4272062443 / dev acc:0.830999970436test acc:0.831499993801\n",
      "train loss: 108.966360759 / dev loss: 34.3264336139/ test loss:33.0233236998 / dev acc:0.83259999752test acc:0.83269995451\n",
      "train loss: 112.755986178 / dev loss: 33.0980727822/ test loss:32.0825005472 / dev acc:0.838999986649test acc:0.833999991417\n",
      "train loss: 111.593230549 / dev loss: 33.594738394/ test loss:32.4271244779 / dev acc:0.835999965668test acc:0.839100003242\n",
      "train loss: 113.775268637 / dev loss: 32.6720379889/ test loss:32.0053973049 / dev acc:0.837699949741test acc:0.837699949741\n",
      "('epoch:', '13')\n",
      "train loss: 100.733207174 / dev loss: 33.7053302228/ test loss:32.8795096204 / dev acc:0.837699949741test acc:0.83159995079\n",
      "train loss: 99.9014419094 / dev loss: 32.902954042/ test loss:32.3253348023 / dev acc:0.837599992752test acc:0.834299981594\n",
      "train loss: 105.24569017 / dev loss: 34.3933795542/ test loss:33.5192681476 / dev acc:0.833499968052test acc:0.831900000572\n",
      "train loss: 108.177285165 / dev loss: 35.2833414227/ test loss:34.338159509 / dev acc:0.836399972439test acc:0.834100008011\n",
      "train loss: 108.643539386 / dev loss: 33.0444896072/ test loss:32.0992633775 / dev acc:0.835299968719test acc:0.836699962616\n",
      "train loss: 109.395815635 / dev loss: 32.8505556583/ test loss:31.5288105831 / dev acc:0.837199985981test acc:0.837099969387\n",
      "('epoch:', '14')\n",
      "train loss: 98.0595032703 / dev loss: 33.5687659234/ test loss:32.4968778715 / dev acc:0.837300002575test acc:0.836699962616\n",
      "train loss: 98.9969278984 / dev loss: 35.2568795234/ test loss:34.0195635557 / dev acc:0.833299994469test acc:0.830099999905\n",
      "train loss: 100.461459754 / dev loss: 36.029073447/ test loss:35.0735607967 / dev acc:0.838699996471test acc:0.833999991417\n",
      "train loss: 103.007690623 / dev loss: 35.4817035794/ test loss:34.1562206745 / dev acc:0.835900008678test acc:0.835199952126\n",
      "train loss: 104.07141258 / dev loss: 35.2291646153/ test loss:34.3764879815 / dev acc:0.834800004959test acc:0.832899987698\n",
      "train loss: 107.165948633 / dev loss: 34.754841581/ test loss:33.8310700431 / dev acc:0.834999978542test acc:0.836499989033\n",
      "('epoch:', '15')\n",
      "train loss: 93.2317315303 / dev loss: 37.7496557981/ test loss:37.1113394685 / dev acc:0.833499968052test acc:0.831999957561\n",
      "train loss: 96.0069619175 / dev loss: 34.7671605498/ test loss:33.8954203613 / dev acc:0.833199977875test acc:0.837499976158\n",
      "train loss: 98.1997899972 / dev loss: 34.645648405/ test loss:33.6899277568 / dev acc:0.834800004959test acc:0.835500001907\n",
      "train loss: 100.776593141 / dev loss: 35.6927043498/ test loss:35.0381465927 / dev acc:0.834100008011test acc:0.83259999752\n",
      "train loss: 99.3041036651 / dev loss: 34.7310905904/ test loss:34.2069287449 / dev acc:0.837399959564test acc:0.838199973106\n",
      "train loss: 103.099121001 / dev loss: 34.2549330741/ test loss:33.0240044259 / dev acc:0.837599992752test acc:0.837999999523\n",
      "max dev acc:0.840299963951/ max test acc: 0.838199973106\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'int' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-79fee704bfa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-79fee704bfa6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/BIBPM_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate 'str' and 'int' objects"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = (Siamese(args, data,use_attention = True))\n",
    "    if args.gpu > -1:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc = 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', str(present_epoch + 1))\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "\n",
    "        # limit the lengths of input sentences up to max_sent_len\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda()\n",
    "                char_h = char_h.cuda()\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = (model(**kwargs))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(pred.shape, batch.label.shape)\n",
    "        batch_loss = criterion(pred.view(-1,2), batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        del pred\n",
    "        del batch_loss\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, args, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print('train loss: '+ str(loss) +' / dev loss: '+ str(dev_loss) + '/ test loss:' + str(test_loss) +\n",
    "                  ' / dev acc:' + str(dev_acc) + 'test acc:' + str(test_acc))\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print('max dev acc:'+ str(max_dev_acc) + '/ max test acc: ' + str(max_test_acc))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=128, type=int)\n",
    "    parser.add_argument('--char-dim', default=20, type=int)\n",
    "    parser.add_argument('--char-hidden-size', default=50, type=int)\n",
    "    parser.add_argument('--data-type', default='Quora', help='available: SNLI or Quora')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=15, type=int)\n",
    "    parser.add_argument('--gpu', default=2, type=int)\n",
    "    parser.add_argument('--hidden-size', default=100, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.001, type=float)\n",
    "    parser.add_argument('--max-sent-len', default=-1, type=int,\n",
    "                        help='max length of input sentences model can accept, if -1, it accepts any length')\n",
    "    parser.add_argument('--num-perspective', default=20, type=int)\n",
    "    parser.add_argument('--print-freq', default=500, type=int)\n",
    "    parser.add_argument('--use-char-emb', default=False, action='store_true')\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "    parser.add_argument('--training', default=0, type=int)\n",
    "    args = parser.parse_args()\n",
    "    print(args.training)\n",
    "    if args.data_type == 'SNLI':\n",
    "        print('loading SNLI data...')\n",
    "        data = SNLI(args)\n",
    "    elif args.data_type == 'Quora':\n",
    "        print('loading Quora data...')\n",
    "        data = Quora(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
    "\n",
    "    setattr(args, 'char_vocab_size', len(data.char_vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'max_word_len', data.max_word_len)\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), 'saved_models/BIBPM_'+args.data_type+'_'+args.model_time+'train'+args.training+'.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1, bidirectional=True)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "c0 = Variable(torch.randn(2, 3, 20))\n",
    "output, hn = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] =str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpython",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
