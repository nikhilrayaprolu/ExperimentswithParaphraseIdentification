{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import attention.transformer.Constants as Constants\n",
    "from attention.transformer.Modules import BottleLinear as Linear\n",
    "from attention.transformer.Layers import EncoderLayer, DecoderLayer\n",
    "\n",
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "\n",
    "def get_attn_padding_mask(seq_q, seq_k):\n",
    "    ''' Indicate the padding-related part to mask '''\n",
    "    assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
    "    mb_size, len_q = seq_q.size()\n",
    "    mb_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(Constants.PAD).unsqueeze(1)   # bx1xsk\n",
    "    pad_attn_mask = pad_attn_mask.expand(mb_size, len_q, len_k) # bxsqxsk\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    ''' Get an attention mask to avoid using the subsequent info.'''\n",
    "    assert seq.dim() == 2\n",
    "    attn_shape = (seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask)\n",
    "    if seq.is_cuda:\n",
    "        subsequent_mask = subsequent_mask.cuda()\n",
    "    return subsequent_mask\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,data, args, n_layers=6, n_head=6, d_k=50, d_v=50,\n",
    "            d_word_vec=300, d_model=300, d_inner_hid=600, dropout=0.1, ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.position_enc = nn.Embedding(100, d_word_vec, padding_idx=Constants.PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(100, d_word_vec)\n",
    "        self.src_word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        #print(data.TEXT.vocab.vectors)\n",
    "        nn.init.uniform(self.src_word_emb.weight.data[0], -0.1, 0.1)\n",
    "        self.src_word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        #self.src_word_emb.weight.requires_grad = False\n",
    "        self.position_enc.weight.requires_grad = False\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos, return_attns=False):\n",
    "        #print(src_seq)\n",
    "        enc_input = self.src_word_emb(src_seq)\n",
    "\n",
    "        # Position Encoding addition\n",
    "        enc_input += self.position_enc(src_pos)\n",
    "        if return_attns:\n",
    "            enc_slf_attns = []\n",
    "\n",
    "        enc_output = enc_input\n",
    "        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output, slf_attn_mask=enc_slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attns += [enc_slf_attn]\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attns\n",
    "        else:\n",
    "            return enc_output,\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "#from attention.transformer.Models import Encoder\n",
    "\n",
    "def test(model, args, data, mode='test'):\n",
    "    if mode == 'dev':\n",
    "        iterator = iter(data.dev_iter)\n",
    "    else:\n",
    "        iterator = iter(data.test_iter)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    acc, loss, size = 0, 0, 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "        \n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda()\n",
    "                char_h = char_h.cuda()\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = model(**kwargs)\n",
    "        #print(pred)\n",
    "        pred = pred.view(-1,2)\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        #print(batch_loss.shape)\n",
    "        loss += batch_loss.data[0]\n",
    "        #print()\n",
    "        _, pred = pred.max(dim=1)\n",
    "        #print(pred)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "    #print(acc, size)\n",
    "    acc /= size\n",
    "    acc = acc.cpu().data[0]\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, args, data, use_attention = False):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n",
    "        self.l = self.args.num_perspective\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n",
    "        self.char_emb.weight.requires_grad = False\n",
    "        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        # no fine-tuning for word vectors\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        self.trainingtype = args.training\n",
    "        self.encoder = Encoder(\n",
    "            data, args)\n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 2, self.args.hidden_size * 1)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 1, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Word Representation Layer -----\n",
    "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
    "        # zero vectors for padding\n",
    "        self.char_emb.weight.data[0].fill_(0)\n",
    "\n",
    "        # <unk> vectors is randomly initialized\n",
    "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
    "\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        p = self.word_emb(kwargs['p'])\n",
    "        h = self.word_emb(kwargs['h'])\n",
    "        #print(kwargs['p'].shape, kwargs['h'].shape)\n",
    "        p_pos = (torch.arange(0,p.shape[1]).view(1,-1).expand(p.shape[0],p.shape[1])).type(torch.LongTensor).cuda()\n",
    "        h_pos = (torch.arange(0,h.shape[1]).view(1,-1).expand(h.shape[0],h.shape[1])).type(torch.LongTensor).cuda()\n",
    "        p_enc_output, *_ = self.encoder(kwargs['p'], p_pos)\n",
    "        h_enc_output, *_ = self.encoder(kwargs['h'], h_pos)\n",
    "        \n",
    "        #print(p_enc_output[1], h_enc_output[1])\n",
    "        \n",
    "        #p_enc_output = p\n",
    "        #h_enc_output = h\n",
    "        p_enc_output_mean = torch.mean(p_enc_output, 1, True)\n",
    "        h_enc_output_mean = torch.mean(h_enc_output, 1, True)\n",
    "        #print(p_enc_output_mean, h_enc_output_mean)\n",
    "        x = torch.cat(\n",
    "            [h_enc_output_mean,p_enc_output_mean], dim=-1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading Quora data...\n",
      "training start!\n",
      "Siamese(\n",
      "  (char_emb): Embedding(1, 20, padding_idx=0)\n",
      "  (word_emb): Embedding(131255, 300)\n",
      "  (encoder): Encoder(\n",
      "    (position_enc): Embedding(100, 300, padding_idx=0)\n",
      "    (src_word_emb): Embedding(131255, 300)\n",
      "    (layer_stack): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d (300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d (600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d (300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d (600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d (300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d (600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d (300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d (600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d (300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d (600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d (300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d (600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pred_fc1): Linear(in_features=600, out_features=300)\n",
      "  (pred_fc2): Linear(in_features=300, out_features=2)\n",
      ")\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav.appidi/new/lib/python3.5/site-packages/torch/nn/modules/module.py:325: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "import torch.nn.functional as F\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = (Siamese(args, data,use_attention = True))\n",
    "    if args.gpu > -1:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(model)\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc, acc, size = 0, 0, 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', str(present_epoch + 1))\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        #print(s1.shape, s2.shape)\n",
    "\n",
    "        # limit the lengths of input sentences up to max_sent_len\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda()\n",
    "                char_h = char_h.cuda()\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = (model(**kwargs))\n",
    "        #print(pred)\n",
    "        optimizer.zero_grad()\n",
    "        #print(pred.view(-1,2), batch.label.shape)\n",
    "        batch_loss = criterion(pred.view(-1,2), batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        a = list(model.parameters())[5].clone()\n",
    "        batch_loss.backward()\n",
    "        #print(list(model.parameters())[5].grad)\n",
    "        #for param in list(model.parameters()):\n",
    "        #    if param.requires_grad:\n",
    "        #        print(param.grad.sum())\n",
    "        #print(pred.view(-1,2).max(dim=1))\n",
    "        _, pred = pred.view(-1,2).max(dim=1)\n",
    "        ones = pred.sum()\n",
    "        #print(ones)\n",
    "        #if ones > 1:\n",
    "        #    print(\"Working Bro!\")\n",
    "        #print(pred)\n",
    "        #print(pred.shape, batch.label.shape)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "        optimizer.step()\n",
    "        b = list(model.parameters())[5].clone()\n",
    "        #print(torch.equal(a.data, b.data))\n",
    "        del pred\n",
    "        del batch_loss\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, args, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('acc/train', acc/size, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print('train loss: '+ str(loss) + 'acc/train' + str((acc/float(size)).data.cpu().numpy()) + ' / dev loss: '+ str(dev_loss) + '/ test loss:' + str(test_loss) +\n",
    "                  ' / dev acc:' + str(dev_acc) + 'test acc:' + str(test_acc))\n",
    "            acc = 0\n",
    "            size = 0\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print('max dev acc:'+ str(max_dev_acc) + '/ max test acc: ' + str(max_test_acc))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=64, type=int)\n",
    "    parser.add_argument('--char-dim', default=20, type=int)\n",
    "    parser.add_argument('--char-hidden-size', default=50, type=int)\n",
    "    parser.add_argument('--data-type', default='Quora', help='available: SNLI or Quora')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=15, type=int)\n",
    "    parser.add_argument('--gpu', default=0, type=int)\n",
    "    parser.add_argument('--hidden-size', default=300, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.0000001, type=float)\n",
    "    parser.add_argument('--max-sent-len', default=100, type=int,\n",
    "                        help='max length of input sentences model can accept, if -1, it accepts any length')\n",
    "    parser.add_argument('--num-perspective', default=20, type=int)\n",
    "    parser.add_argument('--print-freq', default=100, type=int)\n",
    "    parser.add_argument('--use-char-emb', default=False, action='store_true')\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "    parser.add_argument('--training', default=0, type=int)\n",
    "    args = parser.parse_args()\n",
    "    print(args.training)\n",
    "    if args.data_type == 'SNLI':\n",
    "        print('loading SNLI data...')\n",
    "        data = SNLI(args)\n",
    "    elif args.data_type == 'Quora':\n",
    "        print('loading Quora data...')\n",
    "        data = Quora(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
    "\n",
    "    setattr(args, 'char_vocab_size', len(data.char_vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'max_word_len', data.max_word_len)\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), 'saved_models/BIBPM_'+args.data_type+'_'+args.model_time+'train'+args.training+'.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
