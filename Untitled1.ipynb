{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import attention.transformer.Constants as Constants\n",
    "from attention.transformer.Modules import BottleLinear as Linear\n",
    "from attention.transformer.Layers import EncoderLayer, DecoderLayer\n",
    "\n",
    "def position_encoding_init(n_position, d_pos_vec):\n",
    "    ''' Init the sinusoid position encoding table '''\n",
    "\n",
    "    # keep dim 0 for padding token position encoding zero vector\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
    "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
    "\n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "\n",
    "def get_attn_padding_mask(seq_q, seq_k):\n",
    "    ''' Indicate the padding-related part to mask '''\n",
    "    assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
    "    mb_size, len_q = seq_q.size()\n",
    "    mb_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(Constants.PAD).unsqueeze(1)   # bx1xsk\n",
    "    pad_attn_mask = pad_attn_mask.expand(mb_size, len_q, len_k) # bxsqxsk\n",
    "    return pad_attn_mask\n",
    "\n",
    "def get_attn_subsequent_mask(seq):\n",
    "    ''' Get an attention mask to avoid using the subsequent info.'''\n",
    "    assert seq.dim() == 2\n",
    "    attn_shape = (seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask)\n",
    "    if seq.is_cuda:\n",
    "        subsequent_mask = subsequent_mask.cuda()\n",
    "    return subsequent_mask\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,data, args, n_layers=6, n_head=6, d_k=50, d_v=50,\n",
    "            d_word_vec=300, d_model=300, d_inner_hid=600, dropout=0.1, ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.position_enc = nn.Embedding(100, d_word_vec, padding_idx=Constants.PAD)\n",
    "        self.position_enc.weight.data = position_encoding_init(100, d_word_vec)\n",
    "        self.src_word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        #print(data.TEXT.vocab.vectors)\n",
    "        nn.init.uniform(self.src_word_emb.weight.data[0], -0.1, 0.1)\n",
    "        self.src_word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        #self.src_word_emb.weight.requires_grad = False\n",
    "        self.position_enc.weight.requires_grad = False\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, return_attns=False):\n",
    "        #print(src_seq)\n",
    "        enc_input = self.src_word_emb(src_seq)\n",
    "        src_pos = (torch.arange(0,enc_input.shape[1]).view(1,-1).expand(enc_input.shape[0],enc_input.shape[1])).type(torch.LongTensor).cuda()\n",
    "        # Position Encoding addition\n",
    "        enc_input += self.position_enc(src_pos)\n",
    "        if return_attns:\n",
    "            enc_slf_attns = []\n",
    "\n",
    "        enc_output = enc_input\n",
    "        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output, slf_attn_mask=enc_slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attns += [enc_slf_attn]\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attns\n",
    "        else:\n",
    "            return enc_output,\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "#from attention.transformer.Models import Encoder\n",
    "\n",
    "def test(model, args, data, mode='test'):\n",
    "    if mode == 'dev':\n",
    "        iterator = iter(data.dev_iter)\n",
    "    else:\n",
    "        iterator = iter(data.test_iter)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    acc, loss, size = 0, 0, 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "        \n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda()\n",
    "                char_h = char_h.cuda()\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = model(**kwargs)\n",
    "        #print(pred)\n",
    "        pred = pred.view(-1,2)\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        #print(batch_loss.shape)\n",
    "        loss += batch_loss.data[0]\n",
    "        #print()\n",
    "        _, pred = pred.max(dim=1)\n",
    "        #print(pred)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "    #print(acc, size)\n",
    "    acc /= size\n",
    "    acc = acc.cpu().data[0]\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, args, data, use_attention = False):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.trainingtype = args.training\n",
    "        self.encoder = Encoder(\n",
    "            data, args)\n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 2, self.args.hidden_size * 1)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 1, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        #print(kwargs['p'].shape, kwargs['h'].shape)\n",
    "        p_enc_output = self.encoder(kwargs['p'])[0]\n",
    "        h_enc_output = self.encoder(kwargs['h'])[0]\n",
    "        #print(p_enc_output.shape, h_enc_output.shape)        \n",
    "        p_enc_output_mean = torch.mean(p_enc_output, 1, True)\n",
    "        h_enc_output_mean = torch.mean(h_enc_output, 1, True)\n",
    "        #print(p_enc_output_mean.shape, h_enc_output_mean.shape)\n",
    "        x = torch.cat(\n",
    "            [h_enc_output_mean,p_enc_output_mean], dim=-1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading Quora data...\n",
      "training start!\n",
      "Siamese(\n",
      "  (encoder): Encoder(\n",
      "    (position_enc): Embedding(100, 300, padding_idx=0)\n",
      "    (src_word_emb): Embedding(131255, 300)\n",
      "    (layer_stack): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d(300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d(600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d(300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d(600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d(300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d(600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d(300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d(600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d(300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d(600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (slf_attn): MultiHeadAttention(\n",
      "          (attention): ScaledDotProductAttention(\n",
      "            (dropout): Dropout(p=0.1)\n",
      "            (softmax): BottleSoftmax()\n",
      "          )\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (proj): BottleLinear(\n",
      "            (linear): Linear(in_features=300, out_features=300, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "        (pos_ffn): PositionwiseFeedForward(\n",
      "          (w_1): Conv1d(300, 600, kernel_size=(1,), stride=(1,))\n",
      "          (w_2): Conv1d(600, 300, kernel_size=(1,), stride=(1,))\n",
      "          (layer_norm): LayerNormalization(\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pred_fc1): Linear(in_features=600, out_features=300, bias=True)\n",
      "  (pred_fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n",
      "#Params: 43.9M\n",
      "('epoch:', '1')\n",
      "train loss: 69.3001001477acc/train[0.55843747] / dev loss: 108.832010269/ test loss:108.830031157 / dev acc:0.494199991226test acc:0.497599989176\n",
      "train loss: 69.2725063562acc/train[0.62171876] / dev loss: 108.833549678/ test loss:108.831238687 / dev acc:0.5test acc:0.500299990177\n",
      "train loss: 69.236033082acc/train[0.64375] / dev loss: 108.835539877/ test loss:108.832895219 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.2119945884acc/train[0.63984376] / dev loss: 108.837609231/ test loss:108.834602416 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.1767708659acc/train[0.64156246] / dev loss: 108.839805186/ test loss:108.83646822 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.1570624709acc/train[0.6328125] / dev loss: 108.842173159/ test loss:108.838504434 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.1218945384acc/train[0.63640624] / dev loss: 108.844669282/ test loss:108.840667844 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.0854292512acc/train[0.64125] / dev loss: 108.847593009/ test loss:108.84318763 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.0563748479acc/train[0.63921875] / dev loss: 108.850876927/ test loss:108.846096158 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.0414524078acc/train[0.6304687] / dev loss: 108.85424161/ test loss:108.849087656 / dev acc:0.5test acc:0.5\n",
      "train loss: 69.006400466acc/train[0.63374996] / dev loss: 108.857574344/ test loss:108.852064073 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.9647979736acc/train[0.63703126] / dev loss: 108.861571252/ test loss:108.855695903 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.9132249951acc/train[0.64203125] / dev loss: 108.866523802/ test loss:108.860190094 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.8983374834acc/train[0.6353125] / dev loss: 108.871216297/ test loss:108.86448139 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.8797089458acc/train[0.63] / dev loss: 108.876268387/ test loss:108.869090736 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.8041261435acc/train[0.6401562] / dev loss: 108.882232547/ test loss:108.874576271 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.8031751513acc/train[0.63171875] / dev loss: 108.88820529/ test loss:108.880091786 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.758035183acc/train[0.6329687] / dev loss: 108.894568741/ test loss:108.886015832 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.6645845175acc/train[0.64421874] / dev loss: 108.902909398/ test loss:108.893841505 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.6625913382acc/train[0.63640624] / dev loss: 108.910351098/ test loss:108.900828421 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.6877476573acc/train[0.624375] / dev loss: 108.917827129/ test loss:108.907857358 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.5676223636acc/train[0.63859373] / dev loss: 108.927469611/ test loss:108.916978598 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.4898061156acc/train[0.6426562] / dev loss: 108.93797195/ test loss:108.926965892 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.4968267679acc/train[0.6351562] / dev loss: 108.948475659/ test loss:108.936964691 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.474902451acc/train[0.63203126] / dev loss: 108.959257305/ test loss:108.94723779 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.3845870495acc/train[0.63874996] / dev loss: 108.97110033/ test loss:108.958570182 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.3401412368acc/train[0.638125] / dev loss: 108.984043777/ test loss:108.9709481 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.3526519537acc/train[0.630625] / dev loss: 108.997412741/ test loss:108.983778656 / dev acc:0.5test acc:0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 68.1533353329acc/train[0.64890623] / dev loss: 109.013250351/ test loss:108.999040902 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.2789044976acc/train[0.62921876] / dev loss: 109.027052939/ test loss:109.012361526 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.1311727166acc/train[0.64] / dev loss: 109.043559134/ test loss:109.028340816 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.1488390565acc/train[0.6328125] / dev loss: 109.0603652/ test loss:109.04461205 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.9937769175acc/train[0.64421874] / dev loss: 109.078583658/ test loss:109.06229049 / dev acc:0.5test acc:0.5\n",
      "train loss: 68.0710266829acc/train[0.6323437] / dev loss: 109.09656316/ test loss:109.079746723 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.8807759881acc/train[0.64484376] / dev loss: 109.116651356/ test loss:109.099259138 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.9700435996acc/train[0.6325] / dev loss: 109.137271702/ test loss:109.119311273 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.7694814801acc/train[0.64515626] / dev loss: 109.160030067/ test loss:109.141498089 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.7178357244acc/train[0.64515626] / dev loss: 109.185216844/ test loss:109.166081071 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.7215858698acc/train[0.6407812] / dev loss: 109.209320009/ test loss:109.189609885 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.6388472915acc/train[0.6429687] / dev loss: 109.233902514/ test loss:109.213656306 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.6176337004acc/train[0.6404687] / dev loss: 109.259964883/ test loss:109.239162087 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.4567772746acc/train[0.6479687] / dev loss: 109.28946203/ test loss:109.268056214 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.5153692365acc/train[0.640625] / dev loss: 109.316997886/ test loss:109.29504627 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.33329916acc/train[0.64921874] / dev loss: 109.349697053/ test loss:109.327109337 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.6403680444acc/train[0.6265625] / dev loss: 109.373965621/ test loss:109.350934625 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.3984747529acc/train[0.63921875] / dev loss: 109.405404091/ test loss:109.381828427 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.2095553279acc/train[0.6478125] / dev loss: 109.436596215/ test loss:109.412503242 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.3782886863acc/train[0.6354687] / dev loss: 109.468854606/ test loss:109.444262147 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.2881365418acc/train[0.6375] / dev loss: 109.500236273/ test loss:109.475168765 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.2528290749acc/train[0.63734376] / dev loss: 109.534916222/ test loss:109.509338021 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.2683546543acc/train[0.63406247] / dev loss: 109.564316332/ test loss:109.538310468 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.2481620908acc/train[0.63328123] / dev loss: 109.598089635/ test loss:109.571614385 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.1265520453acc/train[0.6378125] / dev loss: 109.631560862/ test loss:109.604656696 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.1955106854acc/train[0.6325] / dev loss: 109.66343838/ test loss:109.636120856 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.9408587813acc/train[0.6426562] / dev loss: 109.702085435/ test loss:109.674271643 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.0676959753acc/train[0.6348437] / dev loss: 109.738463283/ test loss:109.710192144 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.1245856881acc/train[0.6301562] / dev loss: 109.772544563/ test loss:109.743861258 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.1044659019acc/train[0.6303125] / dev loss: 109.806278765/ test loss:109.77719301 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.7861309648acc/train[0.6425] / dev loss: 109.847450554/ test loss:109.817891955 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.6875948906acc/train[0.645] / dev loss: 109.887948334/ test loss:109.857936025 / dev acc:0.5test acc:0.5\n",
      "('epoch:', '2')\n",
      "train loss: 66.6157941818acc/train[0.6459774] / dev loss: 109.933744013/ test loss:109.903239012 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.884478271acc/train[0.63359374] / dev loss: 109.971849144/ test loss:109.940946758 / dev acc:0.5test acc:0.5\n",
      "train loss: 67.0741205215acc/train[0.624375] / dev loss: 110.006455123/ test loss:109.975208879 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.9171257615acc/train[0.6296875] / dev loss: 110.039950311/ test loss:110.00836724 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.5345432162acc/train[0.64375] / dev loss: 110.084099174/ test loss:110.052063942 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.8659697175acc/train[0.62953126] / dev loss: 110.12464726/ test loss:110.092223287 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.6164526939acc/train[0.6378125] / dev loss: 110.165793598/ test loss:110.132975757 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.7376526594acc/train[0.6321875] / dev loss: 110.200033605/ test loss:110.166904032 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.3450443745acc/train[0.6454687] / dev loss: 110.248528004/ test loss:110.214942992 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.347725153acc/train[0.64484376] / dev loss: 110.30129236/ test loss:110.267244637 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.9014741182acc/train[0.62312496] / dev loss: 110.333477497/ test loss:110.299157679 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.2235081792acc/train[0.64687496] / dev loss: 110.380705893/ test loss:110.345994055 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.6754195094acc/train[0.62921876] / dev loss: 110.417756796/ test loss:110.382719159 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.5488478541acc/train[0.6334375] / dev loss: 110.460230291/ test loss:110.424844205 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1677068472acc/train[0.6454687] / dev loss: 110.509715855/ test loss:110.473930538 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.3895353675acc/train[0.63671875] / dev loss: 110.55569464/ test loss:110.519551814 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1096009612acc/train[0.64515626] / dev loss: 110.605564177/ test loss:110.569039464 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.2738282681acc/train[0.63890624] / dev loss: 110.652988791/ test loss:110.616098046 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.5165709853acc/train[0.6304687] / dev loss: 110.688313365/ test loss:110.65116632 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1997368336acc/train[0.6401562] / dev loss: 110.735504985/ test loss:110.698015749 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.2961158156acc/train[0.6359375] / dev loss: 110.779682577/ test loss:110.74188149 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9590801597acc/train[0.6457812] / dev loss: 110.830339015/ test loss:110.792164564 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9759984016acc/train[0.64437497] / dev loss: 110.88192898/ test loss:110.843398273 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7662369609acc/train[0.64984375] / dev loss: 110.935342968/ test loss:110.896436691 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1576330066acc/train[0.6375] / dev loss: 110.981280148/ test loss:110.942061543 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9004668593acc/train[0.64437497] / dev loss: 111.026378036/ test loss:110.986871779 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1749339104acc/train[0.6357812] / dev loss: 111.074344397/ test loss:111.034517169 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.0632571578acc/train[0.6384375] / dev loss: 111.120137632/ test loss:111.080015779 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9797102213acc/train[0.6401562] / dev loss: 111.167512596/ test loss:111.127077341 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.5968679786acc/train[0.6223437] / dev loss: 111.193264246/ test loss:111.152673006 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1053118706acc/train[0.6357812] / dev loss: 111.239953518/ test loss:111.199079573 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9753371477acc/train[0.639375] / dev loss: 111.277009308/ test loss:111.235923111 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1542842388acc/train[0.63328123] / dev loss: 111.323478997/ test loss:111.282114446 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8947408199acc/train[0.64] / dev loss: 111.370553255/ test loss:111.328899503 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.04522717acc/train[0.6357812] / dev loss: 111.409225583/ test loss:111.367334902 / dev acc:0.5test acc:0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 66.1234352589acc/train[0.6328125] / dev loss: 111.447128654/ test loss:111.40501219 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1558841467acc/train[0.63203126] / dev loss: 111.489654124/ test loss:111.447298944 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.0690426826acc/train[0.634375] / dev loss: 111.532283902/ test loss:111.48968792 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.0127856731acc/train[0.6351562] / dev loss: 111.567261934/ test loss:111.52447629 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6972618699acc/train[0.6429687] / dev loss: 111.617372453/ test loss:111.574294388 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6229566932acc/train[0.64484376] / dev loss: 111.664853036/ test loss:111.621500194 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8211807013acc/train[0.63906246] / dev loss: 111.708527923/ test loss:111.664925337 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3674939871acc/train[0.650625] / dev loss: 111.760278583/ test loss:111.716382921 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1220601201acc/train[0.63093746] / dev loss: 111.79799819/ test loss:111.753903091 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.008872509acc/train[0.633125] / dev loss: 111.833880365/ test loss:111.789584696 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3364703059acc/train[0.6503125] / dev loss: 111.882519841/ test loss:111.837964952 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9949613214acc/train[0.63328123] / dev loss: 111.922860622/ test loss:111.878090918 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3813419342acc/train[0.648125] / dev loss: 111.978237331/ test loss:111.933171749 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6741762757acc/train[0.6403125] / dev loss: 112.018281102/ test loss:111.973005414 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5525562167acc/train[0.6432812] / dev loss: 112.064080179/ test loss:112.018581331 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9462473989acc/train[0.633125] / dev loss: 112.095776796/ test loss:112.050113499 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9724434018acc/train[0.6323437] / dev loss: 112.133154809/ test loss:112.087310851 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9932488799acc/train[0.631875] / dev loss: 112.156270683/ test loss:112.110321403 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8889287114acc/train[0.634375] / dev loss: 112.194315732/ test loss:112.148187995 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9052807689acc/train[0.63374996] / dev loss: 112.227020085/ test loss:112.180724919 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.172960341acc/train[0.6271875] / dev loss: 112.25948745/ test loss:112.213043451 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5384770036acc/train[0.6421875] / dev loss: 112.291467071/ test loss:112.244875491 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6298441291acc/train[0.6396875] / dev loss: 112.332721829/ test loss:112.285929799 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7948570251acc/train[0.635625] / dev loss: 112.367730737/ test loss:112.320769757 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6919545531acc/train[0.6379687] / dev loss: 112.401373029/ test loss:112.3542431 / dev acc:0.5test acc:0.5\n",
      "('epoch:', '3')\n",
      "train loss: 65.4491450787acc/train[0.6418919] / dev loss: 112.443859696/ test loss:112.396531492 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4443047643acc/train[0.6434375] / dev loss: 112.483322561/ test loss:112.435808927 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6056820154acc/train[0.63921875] / dev loss: 112.521721065/ test loss:112.474022329 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8632487655acc/train[0.633125] / dev loss: 112.549833417/ test loss:112.50200671 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3563424349acc/train[0.64453125] / dev loss: 112.585689485/ test loss:112.537703246 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.133929193acc/train[0.626875] / dev loss: 112.608282447/ test loss:112.560181946 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1869907379acc/train[0.625625] / dev loss: 112.628913164/ test loss:112.58072263 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7570617795acc/train[0.6351562] / dev loss: 112.657045245/ test loss:112.608733624 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3165822029acc/train[0.64484376] / dev loss: 112.692543626/ test loss:112.644066304 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.20160532acc/train[0.6471875] / dev loss: 112.741816401/ test loss:112.693110198 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8195829988acc/train[0.6334375] / dev loss: 112.771494508/ test loss:112.722639889 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.0307344198acc/train[0.62859374] / dev loss: 112.787393153/ test loss:112.738475323 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.0187987089acc/train[0.62874997] / dev loss: 112.802656114/ test loss:112.753669024 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8457240462acc/train[0.6326562] / dev loss: 112.831508696/ test loss:112.782391518 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1240552366acc/train[0.6484375] / dev loss: 112.8677634/ test loss:112.818478227 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4703044891acc/train[0.6407812] / dev loss: 112.902766943/ test loss:112.853324115 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2661921084acc/train[0.6446875] / dev loss: 112.939603031/ test loss:112.889992714 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7679021657acc/train[0.63374996] / dev loss: 112.968111515/ test loss:112.91835928 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8203360438acc/train[0.6326562] / dev loss: 112.987683833/ test loss:112.937847823 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8179410696acc/train[0.6328125] / dev loss: 113.010564208/ test loss:112.960623533 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1773938537acc/train[0.64640623] / dev loss: 113.045788646/ test loss:112.995689452 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2955316901acc/train[0.64375] / dev loss: 113.086446345/ test loss:113.036171526 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2625828683acc/train[0.64437497] / dev loss: 113.114991605/ test loss:113.064603567 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.54329741acc/train[0.6382812] / dev loss: 113.147827744/ test loss:113.097316504 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5621589422acc/train[0.6376562] / dev loss: 113.182942927/ test loss:113.13228187 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.317340523acc/train[0.6429687] / dev loss: 113.208372951/ test loss:113.157605916 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9358298481acc/train[0.6298437] / dev loss: 113.225168347/ test loss:113.174338251 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.7849469185acc/train[0.6534375] / dev loss: 113.268655062/ test loss:113.217628509 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1084094644acc/train[0.62578124] / dev loss: 113.282781243/ test loss:113.231689095 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1340909302acc/train[0.64625] / dev loss: 113.314926744/ test loss:113.263694078 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6253296137acc/train[0.63609374] / dev loss: 113.331769049/ test loss:113.280470431 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2371194661acc/train[0.6435937] / dev loss: 113.367097855/ test loss:113.3156479 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.2734004259acc/train[0.6225] / dev loss: 113.366861522/ test loss:113.315417856 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2442101836acc/train[0.64375] / dev loss: 113.398127854/ test loss:113.346562207 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1824088097acc/train[0.64484376] / dev loss: 113.433333576/ test loss:113.381614894 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5168995261acc/train[0.6379687] / dev loss: 113.45146668/ test loss:113.399670988 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6599304676acc/train[0.6346875] / dev loss: 113.47366935/ test loss:113.42177707 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1281777322acc/train[0.625] / dev loss: 113.470148087/ test loss:113.418278605 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4898825884acc/train[0.638125] / dev loss: 113.496507108/ test loss:113.444525123 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5585489869acc/train[0.63703126] / dev loss: 113.517801285/ test loss:113.465734303 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5912487209acc/train[0.6357812] / dev loss: 113.529356956/ test loss:113.477244228 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7861732543acc/train[0.6325] / dev loss: 113.541325808/ test loss:113.489161253 / dev acc:0.5test acc:0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 65.3961638808acc/train[0.6403125] / dev loss: 113.567207515/ test loss:113.514924884 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.9918985963acc/train[0.648125] / dev loss: 113.596684396/ test loss:113.544280171 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4154054224acc/train[0.639375] / dev loss: 113.617609501/ test loss:113.565119594 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1417534351acc/train[0.625] / dev loss: 113.62215656/ test loss:113.569641024 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4269461036acc/train[0.63906246] / dev loss: 113.636759996/ test loss:113.584183782 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.4466111362acc/train[0.6190625] / dev loss: 113.639410317/ test loss:113.58683905 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2041012645acc/train[0.6435937] / dev loss: 113.651118934/ test loss:113.598497123 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7742214501acc/train[0.63203126] / dev loss: 113.661830425/ test loss:113.609161675 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2894676328acc/train[0.64203125] / dev loss: 113.689357996/ test loss:113.636574537 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4101178646acc/train[0.63953125] / dev loss: 113.70639801/ test loss:113.653542161 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5534826815acc/train[0.63640624] / dev loss: 113.726062834/ test loss:113.673112333 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8214506507acc/train[0.63124996] / dev loss: 113.731705397/ test loss:113.678730488 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4576207399acc/train[0.63859373] / dev loss: 113.741164923/ test loss:113.688138932 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1283844113acc/train[0.6446875] / dev loss: 113.776849985/ test loss:113.723686188 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6930916011acc/train[0.63374996] / dev loss: 113.78702575/ test loss:113.733819216 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2529780269acc/train[0.6425] / dev loss: 113.805105805/ test loss:113.75181976 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5156755447acc/train[0.6371875] / dev loss: 113.823275149/ test loss:113.769905061 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7618499696acc/train[0.6321875] / dev loss: 113.827133179/ test loss:113.773747861 / dev acc:0.5test acc:0.5\n",
      "('epoch:', '4')\n",
      "train loss: 65.4680692554acc/train[0.63639224] / dev loss: 113.845246315/ test loss:113.79177022 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4441934526acc/train[0.6384375] / dev loss: 113.860312581/ test loss:113.806760967 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1213971078acc/train[0.64484376] / dev loss: 113.881890029/ test loss:113.8282502 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3515273333acc/train[0.63984376] / dev loss: 113.891436517/ test loss:113.837762952 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1295542717acc/train[0.64406246] / dev loss: 113.917263478/ test loss:113.863476992 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2587780952acc/train[0.64187497] / dev loss: 113.935168684/ test loss:113.881298333 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.9877311885acc/train[0.6471875] / dev loss: 113.957171589/ test loss:113.903202325 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1466753185acc/train[0.6245312] / dev loss: 113.950649261/ test loss:113.8967112 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7713755369acc/train[0.631875] / dev loss: 113.957746327/ test loss:113.903781772 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.0496777296acc/train[0.62624997] / dev loss: 113.958953142/ test loss:113.904994965 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4712868035acc/train[0.63734376] / dev loss: 113.962011397/ test loss:113.908042192 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7501759529acc/train[0.631875] / dev loss: 113.971452504/ test loss:113.917443365 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6306507587acc/train[0.6346875] / dev loss: 113.97011736/ test loss:113.916104406 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6428852677acc/train[0.63421875] / dev loss: 113.975335091/ test loss:113.921301037 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5009652674acc/train[0.63671875] / dev loss: 113.990712732/ test loss:113.936605901 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.3205610812acc/train[0.6403125] / dev loss: 113.992543429/ test loss:113.938432217 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.0451898575acc/train[0.6454687] / dev loss: 114.028283417/ test loss:113.974018186 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.0413443446acc/train[0.6457812] / dev loss: 114.05782944/ test loss:114.003421038 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.6494680643acc/train[0.63390625] / dev loss: 114.056216985/ test loss:114.001817763 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.9519072473acc/train[0.628125] / dev loss: 114.04733175/ test loss:113.992965072 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1137979031acc/train[0.64406246] / dev loss: 114.07388553/ test loss:114.019401461 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.8624113202acc/train[0.64875] / dev loss: 114.100308418/ test loss:114.045696795 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5264413357acc/train[0.63624996] / dev loss: 114.10139361/ test loss:114.046765238 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.920850724acc/train[0.62843746] / dev loss: 114.098367631/ test loss:114.043740898 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.2240332067acc/train[0.64171875] / dev loss: 114.11441496/ test loss:114.059711248 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8378808498acc/train[0.6304687] / dev loss: 114.110258847/ test loss:114.055571735 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4849143028acc/train[0.636875] / dev loss: 114.112068772/ test loss:114.057351857 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1479554474acc/train[0.6434375] / dev loss: 114.121930182/ test loss:114.067166865 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7286719084acc/train[0.6321875] / dev loss: 114.123290151/ test loss:114.068511069 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.9981777072acc/train[0.6459375] / dev loss: 114.14180702/ test loss:114.086955339 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.0758629441acc/train[0.64437497] / dev loss: 114.171607614/ test loss:114.116622746 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4437707961acc/train[0.63734376] / dev loss: 114.180155516/ test loss:114.125117809 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5719000101acc/train[0.6353125] / dev loss: 114.18008256/ test loss:114.125042766 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.1750153899acc/train[0.64234376] / dev loss: 114.195471376/ test loss:114.140349358 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5401015878acc/train[0.63609374] / dev loss: 114.195984274/ test loss:114.140851259 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.4130934477acc/train[0.638125] / dev loss: 114.199331462/ test loss:114.144159347 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.8758067787acc/train[0.62921876] / dev loss: 114.185568422/ test loss:114.130446076 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.6475302577acc/train[0.65234375] / dev loss: 114.22242704/ test loss:114.167140275 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.7942987978acc/train[0.63093746] / dev loss: 114.213890851/ test loss:114.158626825 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.7569399178acc/train[0.65015626] / dev loss: 114.236509264/ test loss:114.181129307 / dev acc:0.5test acc:0.5\n",
      "train loss: 64.9320670366acc/train[0.6471875] / dev loss: 114.251832128/ test loss:114.196371883 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.3100579083acc/train[0.62125] / dev loss: 114.231941313/ test loss:114.176551104 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.0938183665acc/train[0.6434375] / dev loss: 114.256569177/ test loss:114.201062888 / dev acc:0.5test acc:0.5\n",
      "train loss: 66.1759096086acc/train[0.62359375] / dev loss: 114.23175472/ test loss:114.176335931 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.0693864524acc/train[0.64390624] / dev loss: 114.241032302/ test loss:114.185560822 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5120745003acc/train[0.63609374] / dev loss: 114.242612839/ test loss:114.18711251 / dev acc:0.5test acc:0.5\n",
      "train loss: 65.5413701534acc/train[0.6354687] / dev loss: 114.249693483/ test loss:114.194140643 / dev acc:0.5test acc:0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c5617656b9d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-c5617656b9d9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training start!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-c5617656b9d9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, data)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;31m#print(pred.view(-1,2).max(dim=1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav.appidi/nikhil/SummaRuNNer/newpython/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav.appidi/nikhil/SummaRuNNer/newpython/local/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "import torch.nn.functional as F\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = (Siamese(args, data,use_attention = True))\n",
    "    if args.gpu > -1:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(model)\n",
    "    params = sum(p.numel() for p in list(model.parameters())) / 1e6\n",
    "    print('#Params: %.1fM' % (params))\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc, acc, size = 0, 0, 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', str(present_epoch + 1))\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        #print(s1.shape, s2.shape)\n",
    "\n",
    "        # limit the lengths of input sentences up to max_sent_len\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda()\n",
    "                char_h = char_h.cuda()\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = (model(**kwargs))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(pred.view(-1,2), batch.label.shape)\n",
    "        batch_loss = criterion(pred.view(-1,2), batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        a = list(model.parameters())[5].clone()\n",
    "        batch_loss.backward()\n",
    "        #print(pred.view(-1,2).max(dim=1))\n",
    "        _, pred = pred.view(-1,2).max(dim=1)\n",
    "        #ones = pred.sum()\n",
    "        #print(ones)\n",
    "        #if ones > 1:\n",
    "        #    print(\"Working Bro!\")\n",
    "        #print(pred)\n",
    "        #print(pred.shape, batch.label.shape)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "        optimizer.step()\n",
    "        b = list(model.parameters())[5].clone()\n",
    "        #print(torch.equal(a.data, b.data))\n",
    "        del pred\n",
    "        del batch_loss\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, args, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('acc/train', acc/size, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print('train loss: '+ str(loss) + 'acc/train' + str((acc/float(size)).data.cpu().numpy()) + ' / dev loss: '+ str(dev_loss) + '/ test loss:' + str(test_loss) +\n",
    "                  ' / dev acc:' + str(dev_acc) + 'test acc:' + str(test_acc))\n",
    "            acc = 0\n",
    "            size = 0\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print('max dev acc:'+ str(max_dev_acc) + '/ max test acc: ' + str(max_test_acc))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=64, type=int)\n",
    "    parser.add_argument('--char-dim', default=20, type=int)\n",
    "    parser.add_argument('--char-hidden-size', default=50, type=int)\n",
    "    parser.add_argument('--data-type', default='Quora', help='available: SNLI or Quora')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=15, type=int)\n",
    "    parser.add_argument('--gpu', default=0, type=int)\n",
    "    parser.add_argument('--hidden-size', default=300, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.0000001, type=float)\n",
    "    parser.add_argument('--max-sent-len', default=100, type=int,\n",
    "                        help='max length of input sentences model can accept, if -1, it accepts any length')\n",
    "    parser.add_argument('--num-perspective', default=20, type=int)\n",
    "    parser.add_argument('--print-freq', default=100, type=int)\n",
    "    parser.add_argument('--use-char-emb', default=False, action='store_true')\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "    parser.add_argument('--training', default=0, type=int)\n",
    "    args = parser.parse_args()\n",
    "    print(args.training)\n",
    "    if args.data_type == 'SNLI':\n",
    "        print('loading SNLI data...')\n",
    "        data = SNLI(args)\n",
    "    elif args.data_type == 'Quora':\n",
    "        print('loading Quora data...')\n",
    "        data = Quora(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
    "\n",
    "    setattr(args, 'char_vocab_size', len(data.char_vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'max_word_len', data.max_word_len)\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), 'saved_models/BIBPM_'+args.data_type+'_'+args.model_time+'train'+args.training+'.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpython",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
