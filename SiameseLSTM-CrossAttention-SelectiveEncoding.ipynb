{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "def test(model, args, data, mode='test'):\n",
    "    if mode == 'dev':\n",
    "        iterator = iter(data.dev_iter)\n",
    "    else:\n",
    "        iterator = iter(data.test_iter)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    acc, loss, size = 0, 0, 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda(args.gpu)\n",
    "                char_h = char_h.cuda(args.gpu)\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = model(**kwargs)\n",
    "        pred = pred.view(-1,2)\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "\n",
    "        _, pred = pred.max(dim=1)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "\n",
    "    acc /= size\n",
    "    acc = acc.cpu().data[0]\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def load_model(args, data):\n",
    "    model = BIMPM(args, data)\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "\n",
    "    if args.gpu > -1:\n",
    "        model.cuda(args.gpu)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    Examples::\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_out = nn.Linear(dim*2, dim)\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, output, context):\n",
    "        batch_size = output.size(0)\n",
    "        hidden_size = output.size(2)\n",
    "        input_size = context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, input_size)).view(batch_size, -1, input_size)\n",
    "\n",
    "        # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        mix = torch.bmm(attn, context)\n",
    "\n",
    "        # concat -> (batch, out_len, 2*dim)\n",
    "        combined = torch.cat((mix, output), dim=2)\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = F.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return output, attn\n",
    "    \n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, args, data, use_attention = False):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n",
    "        self.l = self.args.num_perspective\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n",
    "\n",
    "        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        # no fine-tuning for word vectors\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        self.trainingtype = args.training\n",
    "        self.use_attention = use_attention\n",
    "        if self.use_attention:\n",
    "            self.attention = Attention(self.args.hidden_size*2)\n",
    "        \n",
    "        self.char_LSTM = nn.LSTM(\n",
    "            input_size=self.args.char_dim,\n",
    "            hidden_size=self.args.char_hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        self.context_LSTM = nn.LSTM(\n",
    "            input_size=self.d,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.aggregation_LSTM = nn.LSTM(\n",
    "            input_size=self.args.hidden_size*2,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.Ws = nn.Parameter(torch.rand(self.args.hidden_size*2,self.args.hidden_size*2))\n",
    "        self.Us = nn.Parameter(torch.rand(self.args.hidden_size*2,self.args.hidden_size*2))\n",
    "        self.bs = nn.Parameter(torch.rand(self.args.hidden_size*2))\n",
    "        \n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 4, self.args.hidden_size * 2)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 2, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Word Representation Layer -----\n",
    "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
    "        # zero vectors for padding\n",
    "        self.char_emb.weight.data[0].fill_(0)\n",
    "\n",
    "        # <unk> vectors is randomly initialized\n",
    "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
    "\n",
    "        nn.init.kaiming_normal(self.char_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.char_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0_reverse, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0_reverse, val=0)\n",
    "\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        p = self.word_emb(kwargs['p'])\n",
    "        h = self.word_emb(kwargs['h'])\n",
    "\n",
    "        if self.args.use_char_emb:\n",
    "            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n",
    "            seq_len_p = kwargs['char_p'].size(1)\n",
    "            seq_len_h = kwargs['char_h'].size(1)\n",
    "\n",
    "            char_p = kwargs['char_p'].view(-1, self.args.max_word_len)\n",
    "            char_h = kwargs['char_h'].view(-1, self.args.max_word_len)\n",
    "\n",
    "            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n",
    "            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n",
    "            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n",
    "\n",
    "            # (batch, seq_len, char_hidden_size)\n",
    "            char_p = char_p.view(-1, seq_len_p, self.args.char_hidden_size)\n",
    "            char_h = char_h.view(-1, seq_len_h, self.args.char_hidden_size)\n",
    "\n",
    "            # (batch, seq_len, word_dim + char_hidden_size)\n",
    "            p = torch.cat([p, char_p], dim=-1)\n",
    "            h = torch.cat([h, char_h], dim=-1)\n",
    "\n",
    "        p = self.dropout(p)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        # (batch, seq_len, hidden_size * 2)\n",
    "        #self.context_LSTM.flatten_parameters()\n",
    "        con_p, _ = self.context_LSTM(p)\n",
    "        con_h, _ = self.context_LSTM(h)\n",
    "        \n",
    "\n",
    "        #print(con_p.shape)\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "        \n",
    "        \n",
    "        p_key = torch.cat([con_p_fw[:,-1,:],con_p_bw[:,0,:]], dim=-1)\n",
    "        h_key = torch.cat([con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        #print(self.Ws.shape, con_p.shape, self.Us.shape, p_key.shape, torch.matmul(con_p,self.Ws).shape , torch.matmul(p_key,self.Us).view(-1,1,self.args.hidden_size*2).expand(con_p.shape[0],con_p.shape[1],con_p.shape[2]).shape , self.bs.shape)\n",
    "        sGatep = F.sigmoid(torch.matmul(con_p,self.Ws) + torch.matmul(h_key,self.Us).view(-1,1,self.args.hidden_size*2).expand(con_p.shape[0],con_p.shape[1],con_p.shape[2]) + self.bs)\n",
    "        sGateh = F.sigmoid(torch.matmul(con_h,self.Ws) + torch.matmul(p_key,self.Us).view(-1,1,self.args.hidden_size*2).expand(con_h.shape[0],con_h.shape[1],con_h.shape[2]) + self.bs)\n",
    "        con_p = sGatep * con_p\n",
    "        con_h = sGateh * con_h\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "        \n",
    "        #con_p, _ = self.aggregation_LSTM(con_p_sel)\n",
    "        #con_h, _ = self.aggregation_LSTM(con_h_sel)\n",
    "        p_key = torch.cat([con_p_fw[:,-1,:],con_p_bw[:,0,:]], dim=-1)\n",
    "        h_key = torch.cat([con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "\n",
    "        #print(h_key.shape, con_p.shape, con_h_fw.shape,con_h_fw[:,-1,:].shape, )\n",
    "        p_attn_output, attn_p = self.attention(h_key.view(-1,1,h_key.shape[1]), con_p)\n",
    "        \n",
    "        h_attn_output, attn_h = self.attention(p_key.view(-1,1,p_key.shape[1]), con_h)\n",
    "        #print(p_attn_output.shape)\n",
    "        x = torch.cat([p_attn_output, h_attn_output], dim=-1)\n",
    "        \n",
    "        #print(con_p_fw[:,-1,:].shape)\n",
    "        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n",
    "        #x = torch.cat(\n",
    "        #    [con_p_fw[:,-1,:],con_p_bw[:,0,:],con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading Quora data...\n",
      "training start!\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "foo:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 286.8325356990099 / dev loss: 44.939575999975204/ test loss:44.50190236419439 / dev acc:0.7020999789237976test acc:0.7067999839782715\n",
      "train loss: 254.34119687974453 / dev loss: 42.23564714193344/ test loss:41.78370562195778 / dev acc:0.7295999526977539test acc:0.7321000099182129\n",
      "train loss: 241.534853130579 / dev loss: 39.25683569908142/ test loss:38.49264515936375 / dev acc:0.7512999773025513test acc:0.7547999620437622\n",
      "train loss: 235.30443413555622 / dev loss: 37.42780585587025/ test loss:36.93216770142317 / dev acc:0.7727999687194824test acc:0.7748000025749207\n",
      "train loss: 227.51774036884308 / dev loss: 37.2180700302124/ test loss:36.975500620901585 / dev acc:0.7716000080108643test acc:0.7678999900817871\n",
      "train loss: 221.00869449973106 / dev loss: 36.06823143362999/ test loss:35.70600912719965 / dev acc:0.7804999947547913test acc:0.7795999646186829\n",
      "epoch: 2\n",
      "train loss: 208.67066250368953 / dev loss: 35.95912329852581/ test loss:34.783963177353144 / dev acc:0.7888000011444092test acc:0.7935000061988831\n",
      "train loss: 206.23321774601936 / dev loss: 34.81638218462467/ test loss:34.31394425034523 / dev acc:0.7917999625205994test acc:0.7985999584197998\n",
      "train loss: 204.37355167418718 / dev loss: 35.511097341775894/ test loss:34.946573596447706 / dev acc:0.7906999588012695test acc:0.7874999642372131\n",
      "train loss: 201.3904116153717 / dev loss: 33.638240441679955/ test loss:33.95624306797981 / dev acc:0.8008999824523926test acc:0.7962999939918518\n",
      "train loss: 200.1931440755725 / dev loss: 34.37532423436642/ test loss:34.3262929841876 / dev acc:0.7928000092506409test acc:0.7931999564170837\n",
      "train loss: 199.88210073113441 / dev loss: 33.933350801467896/ test loss:33.867183558642864 / dev acc:0.7989000082015991test acc:0.7942999601364136\n",
      "epoch: 3\n",
      "train loss: 182.84608159586787 / dev loss: 32.840357944369316/ test loss:32.48012675717473 / dev acc:0.8077999949455261test acc:0.805899977684021\n",
      "train loss: 184.64581498503685 / dev loss: 36.420610427856445/ test loss:35.48104880750179 / dev acc:0.7978000044822693test acc:0.7980999946594238\n",
      "train loss: 181.43846904858947 / dev loss: 32.87703378498554/ test loss:32.67494686320424 / dev acc:0.805899977684021test acc:0.8053999543190002\n",
      "train loss: 183.5087627992034 / dev loss: 32.71033924818039/ test loss:31.705141574144363 / dev acc:0.8125999569892883test acc:0.8165000081062317\n",
      "train loss: 181.3458483144641 / dev loss: 32.54502473771572/ test loss:32.500386126339436 / dev acc:0.8162999749183655test acc:0.8157999515533447\n",
      "train loss: 183.70265397801995 / dev loss: 32.41018325090408/ test loss:31.618233658373356 / dev acc:0.8173999786376953test acc:0.8172999620437622\n",
      "epoch: 4\n",
      "train loss: 166.186218470335 / dev loss: 32.254836574196815/ test loss:31.78575237095356 / dev acc:0.8183000087738037test acc:0.8190999627113342\n",
      "train loss: 166.92814116925 / dev loss: 31.13181161880493/ test loss:30.741257827728987 / dev acc:0.8226000070571899test acc:0.8208999633789062\n",
      "train loss: 168.77835968136787 / dev loss: 33.08247311413288/ test loss:33.02242083847523 / dev acc:0.8127999901771545test acc:0.8097999691963196\n",
      "train loss: 169.2234094887972 / dev loss: 32.45822176337242/ test loss:32.41152173280716 / dev acc:0.8137999773025513test acc:0.8151999711990356\n",
      "train loss: 167.9686174504459 / dev loss: 33.69264820218086/ test loss:33.188110545277596 / dev acc:0.8053999543190002test acc:0.8048999905586243\n",
      "train loss: 167.4982137903571 / dev loss: 31.002197220921516/ test loss:30.062232114374638 / dev acc:0.8217999935150146test acc:0.8258000016212463\n",
      "epoch: 5\n",
      "train loss: 153.35930030047894 / dev loss: 30.911957412958145/ test loss:30.4538336917758 / dev acc:0.8291999697685242test acc:0.8259999752044678\n",
      "train loss: 152.9531974978745 / dev loss: 31.903613179922104/ test loss:31.267087779939175 / dev acc:0.8258000016212463test acc:0.8235999941825867\n",
      "train loss: 155.44188544340432 / dev loss: 30.943582490086555/ test loss:30.13084863871336 / dev acc:0.8258999586105347test acc:0.8296999931335449\n",
      "train loss: 155.7366806268692 / dev loss: 31.544801622629166/ test loss:31.05849200487137 / dev acc:0.8252999782562256test acc:0.82669997215271\n",
      "train loss: 158.59425534307957 / dev loss: 30.970315858721733/ test loss:30.337648563086987 / dev acc:0.8288999795913696test acc:0.8270999789237976\n",
      "train loss: 158.40323965251446 / dev loss: 32.343224093317986/ test loss:31.711375512182713 / dev acc:0.8185999989509583test acc:0.81659996509552\n",
      "epoch: 6\n",
      "train loss: 140.40572087839246 / dev loss: 31.396474987268448/ test loss:30.716262187808752 / dev acc:0.8296999931335449test acc:0.8330999612808228\n",
      "train loss: 141.99795635417104 / dev loss: 32.14079546928406/ test loss:31.21098256856203 / dev acc:0.8282999992370605test acc:0.8295999765396118\n",
      "train loss: 146.2699896618724 / dev loss: 30.822621285915375/ test loss:30.117684304714203 / dev acc:0.8262999653816223test acc:0.828499972820282\n",
      "train loss: 147.58205660805106 / dev loss: 31.246838375926018/ test loss:30.200920447707176 / dev acc:0.8330000042915344test acc:0.8296999931335449\n",
      "train loss: 146.65768042951822 / dev loss: 30.267087765038013/ test loss:29.86917182803154 / dev acc:0.8335999846458435test acc:0.8309999704360962\n",
      "train loss: 147.5321144014597 / dev loss: 32.94857306778431/ test loss:32.60238923877478 / dev acc:0.8298999667167664test acc:0.8276999592781067\n",
      "epoch: 7\n",
      "train loss: 131.34545362368226 / dev loss: 31.930992975831032/ test loss:31.732616305351257 / dev acc:0.8323000073432922test acc:0.8312999606132507\n",
      "train loss: 134.93098620325327 / dev loss: 30.409994021058083/ test loss:29.780122205615044 / dev acc:0.8346999883651733test acc:0.8325999975204468\n",
      "train loss: 135.9018104635179 / dev loss: 31.27435651421547/ test loss:30.64831718057394 / dev acc:0.8301999568939209test acc:0.8343999981880188\n",
      "train loss: 136.688270278275 / dev loss: 31.4105221927166/ test loss:30.81507346779108 / dev acc:0.8288999795913696test acc:0.8263999819755554\n",
      "train loss: 138.48689847812057 / dev loss: 30.890397921204567/ test loss:30.24480477720499 / dev acc:0.837399959564209test acc:0.8341000080108643\n",
      "train loss: 137.37001011148095 / dev loss: 31.644017413258553/ test loss:30.904618322849274 / dev acc:0.8310999870300293test acc:0.8305999636650085\n",
      "epoch: 8\n",
      "train loss: 122.05068932287395 / dev loss: 31.042861953377724/ test loss:30.724086195230484 / dev acc:0.8384000062942505test acc:0.8374999761581421\n",
      "train loss: 125.5311685949564 / dev loss: 31.512787774205208/ test loss:30.91233430057764 / dev acc:0.8307999968528748test acc:0.8331999778747559\n",
      "train loss: 128.07387842610478 / dev loss: 31.479577869176865/ test loss:30.34518074989319 / dev acc:0.8359999656677246test acc:0.8392999768257141\n",
      "train loss: 129.25664533581585 / dev loss: 31.126004211604595/ test loss:30.33663260936737 / dev acc:0.8395999670028687test acc:0.8378999829292297\n",
      "train loss: 131.06830137223005 / dev loss: 31.998640716075897/ test loss:31.478993862867355 / dev acc:0.8314999938011169test acc:0.8328999876976013\n",
      "train loss: 133.41176832467318 / dev loss: 31.414059594273567/ test loss:30.147019788622856 / dev acc:0.8343999981880188test acc:0.8351999521255493\n",
      "epoch: 9\n",
      "train loss: 115.43903350830078 / dev loss: 32.778189197182655/ test loss:31.361419171094894 / dev acc:0.8324999809265137test acc:0.8381999731063843\n",
      "train loss: 118.13262070342898 / dev loss: 34.09247267246246/ test loss:32.942734748125076 / dev acc:0.8297999501228333test acc:0.833299994468689\n",
      "train loss: 120.63829800672829 / dev loss: 33.18715053796768/ test loss:32.11082077771425 / dev acc:0.8301999568939209test acc:0.8330999612808228\n",
      "train loss: 123.70734149590135 / dev loss: 32.43020747601986/ test loss:31.470665156841278 / dev acc:0.8270999789237976test acc:0.8292999863624573\n",
      "train loss: 123.08873343653977 / dev loss: 32.053837567567825/ test loss:31.073198899626732 / dev acc:0.8376999497413635test acc:0.8353999853134155\n",
      "train loss: 125.11563609167933 / dev loss: 31.78334639966488/ test loss:30.914789527654648 / dev acc:0.8297999501228333test acc:0.8269000053405762\n",
      "epoch: 10\n",
      "train loss: 109.38332914561033 / dev loss: 32.63682049512863/ test loss:31.399429380893707 / dev acc:0.8337999582290649test acc:0.8402999639511108\n",
      "train loss: 112.60513319447637 / dev loss: 33.671923991292715/ test loss:32.72055231779814 / dev acc:0.8381999731063843test acc:0.8388999700546265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 114.21192016080022 / dev loss: 32.146309569478035/ test loss:31.661391898989677 / dev acc:0.8335999846458435test acc:0.8348000049591064\n",
      "train loss: 115.92065407335758 / dev loss: 32.45265334844589/ test loss:31.671960093080997 / dev acc:0.8371999859809875test acc:0.8385999798774719\n",
      "train loss: 117.72203773632646 / dev loss: 32.18330378830433/ test loss:31.332482673227787 / dev acc:0.8352999687194824test acc:0.8367999792098999\n",
      "train loss: 121.34399233665317 / dev loss: 32.07465095818043/ test loss:31.11084795743227 / dev acc:0.8363999724388123test acc:0.8413999676704407\n",
      "epoch: 11\n",
      "train loss: 104.68838837556541 / dev loss: 31.599782302975655/ test loss:30.76685406267643 / dev acc:0.8370999693870544test acc:0.8387999534606934\n",
      "train loss: 108.14153987169266 / dev loss: 36.159360092133284/ test loss:34.965596579015255 / dev acc:0.8333999514579773test acc:0.8345999717712402\n",
      "train loss: 109.84660516679287 / dev loss: 33.56406153738499/ test loss:32.152906373143196 / dev acc:0.8334999680519104test acc:0.8348999619483948\n",
      "train loss: 110.3298882972449 / dev loss: 32.37445328757167/ test loss:31.920744001865387 / dev acc:0.8395999670028687test acc:0.8387999534606934\n",
      "train loss: 114.48971608281136 / dev loss: 33.55407336354256/ test loss:32.9193065315485 / dev acc:0.8389999866485596test acc:0.8373000025749207\n",
      "train loss: 115.14869563095272 / dev loss: 33.68856291472912/ test loss:32.67957433313131 / dev acc:0.8359000086784363test acc:0.835599958896637\n",
      "epoch: 12\n",
      "train loss: 100.80050232820213 / dev loss: 37.00051899254322/ test loss:35.67575125396252 / dev acc:0.836899995803833test acc:0.8381999731063843\n",
      "train loss: 102.52988296188414 / dev loss: 36.4141034707427/ test loss:35.70706017315388 / dev acc:0.8279999494552612test acc:0.8280999660491943\n",
      "train loss: 105.72251092456281 / dev loss: 33.681576281785965/ test loss:33.168569065630436 / dev acc:0.8333999514579773test acc:0.8355000019073486\n",
      "train loss: 104.77336614951491 / dev loss: 33.73451352119446/ test loss:32.702496498823166 / dev acc:0.8355000019073486test acc:0.8373000025749207\n",
      "train loss: 108.2042080219835 / dev loss: 34.027713507413864/ test loss:33.49554631114006 / dev acc:0.8345999717712402test acc:0.8356999754905701\n",
      "train loss: 110.00192930921912 / dev loss: 33.81759214401245/ test loss:32.50863240659237 / dev acc:0.8393999934196472test acc:0.8367999792098999\n",
      "epoch: 13\n",
      "train loss: 96.45531698688865 / dev loss: 33.389496713876724/ test loss:31.665606655180454 / dev acc:0.8339999914169312test acc:0.8422999978065491\n",
      "train loss: 97.74659618362784 / dev loss: 36.80874375998974/ test loss:35.46169137209654 / dev acc:0.8312000036239624test acc:0.8349999785423279\n",
      "train loss: 100.60084884054959 / dev loss: 36.08452612161636/ test loss:34.945468574762344 / dev acc:0.8380999565124512test acc:0.839199960231781\n",
      "train loss: 103.02079617418349 / dev loss: 32.38798236846924/ test loss:31.717954218387604 / dev acc:0.8357999920845032test acc:0.8418999910354614\n",
      "train loss: 104.85979270748794 / dev loss: 32.911932326853275/ test loss:32.82954829186201 / dev acc:0.837399959564209test acc:0.8380999565124512\n",
      "train loss: 105.12160092778504 / dev loss: 34.49034032225609/ test loss:33.12160678207874 / dev acc:0.833899974822998test acc:0.8381999731063843\n",
      "epoch: 14\n",
      "train loss: 92.15804453101009 / dev loss: 35.816465586423874/ test loss:34.807454131543636 / dev acc:0.8359000086784363test acc:0.8382999897003174\n",
      "train loss: 97.1253423243761 / dev loss: 35.03318690508604/ test loss:34.77914056181908 / dev acc:0.8287000060081482test acc:0.8274999856948853\n",
      "train loss: 96.52015329711139 / dev loss: 34.72676542401314/ test loss:34.22208707034588 / dev acc:0.8376999497413635test acc:0.8339999914169312\n",
      "train loss: 98.03285849653184 / dev loss: 34.26662819087505/ test loss:34.03246359527111 / dev acc:0.8411999940872192test acc:0.8375999927520752\n",
      "train loss: 102.32651156373322 / dev loss: 34.58291049301624/ test loss:34.26825173199177 / dev acc:0.8362999558448792test acc:0.8361999988555908\n",
      "train loss: 101.49469557218254 / dev loss: 34.2181626111269/ test loss:33.47698670625687 / dev acc:0.839199960231781test acc:0.8361999988555908\n",
      "epoch: 15\n",
      "train loss: 90.32944460678846 / dev loss: 36.594647623598576/ test loss:35.30741559714079 / dev acc:0.8277999758720398test acc:0.8342999815940857\n",
      "train loss: 92.18760953471065 / dev loss: 36.67465711385012/ test loss:35.634326726198196 / dev acc:0.8348999619483948test acc:0.833299994468689\n",
      "train loss: 93.91422046907246 / dev loss: 37.89974734187126/ test loss:36.96841272711754 / dev acc:0.8376999497413635test acc:0.8310999870300293\n",
      "train loss: 96.68054590467364 / dev loss: 33.80028413236141/ test loss:32.979429200291634 / dev acc:0.8362999558448792test acc:0.8359999656677246\n",
      "train loss: 98.0210173651576 / dev loss: 35.10296688973904/ test loss:33.88208329677582 / dev acc:0.8355000019073486test acc:0.8348999619483948\n",
      "train loss: 99.83664413541555 / dev loss: 32.824721574783325/ test loss:32.38240201771259 / dev acc:0.8366000056266785test acc:0.8323999643325806\n",
      "max dev acc:0.8411999940872192/ max test acc: 0.8375999927520752\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'int' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-9ce84a9e1f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-9ce84a9e1f30>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/BIBPM_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'int' object to str implicitly"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = (Siamese(args, data,use_attention = True))\n",
    "    if args.gpu > -1:\n",
    "        model.cuda(args.gpu)\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc = 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', str(present_epoch + 1))\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "\n",
    "        # limit the lengths of input sentences up to max_sent_len\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda(args.gpu)\n",
    "                char_h = char_h.cuda(args.gpu)\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = (model(**kwargs))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(pred.shape, batch.label.shape)\n",
    "        batch_loss = criterion(pred.view(-1,2), batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        del pred\n",
    "        del batch_loss\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, args, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print('train loss: '+ str(loss) +' / dev loss: '+ str(dev_loss) + '/ test loss:' + str(test_loss) +\n",
    "                  ' / dev acc:' + str(dev_acc) + 'test acc:' + str(test_acc))\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print('max dev acc:'+ str(max_dev_acc) + '/ max test acc: ' + str(max_test_acc))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=128, type=int)\n",
    "    parser.add_argument('--char-dim', default=20, type=int)\n",
    "    parser.add_argument('--char-hidden-size', default=50, type=int)\n",
    "    parser.add_argument('--data-type', default='Quora', help='available: SNLI or Quora')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=15, type=int)\n",
    "    parser.add_argument('--gpu', default=1, type=int)\n",
    "    parser.add_argument('--hidden-size', default=100, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.001, type=float)\n",
    "    parser.add_argument('--max-sent-len', default=-1, type=int,\n",
    "                        help='max length of input sentences model can accept, if -1, it accepts any length')\n",
    "    parser.add_argument('--num-perspective', default=20, type=int)\n",
    "    parser.add_argument('--print-freq', default=500, type=int)\n",
    "    parser.add_argument('--use-char-emb', default=False, action='store_true')\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "    parser.add_argument('--training', default=0, type=int)\n",
    "    args = parser.parse_args()\n",
    "    print(args.training)\n",
    "    if args.data_type == 'SNLI':\n",
    "        print('loading SNLI data...')\n",
    "        data = SNLI(args)\n",
    "    elif args.data_type == 'Quora':\n",
    "        print('loading Quora data...')\n",
    "        data = Quora(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
    "\n",
    "    setattr(args, 'char_vocab_size', len(data.char_vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'max_word_len', data.max_word_len)\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), 'saved_models/BIBPM_'+args.data_type+'_'+args.model_time+'train'+args.training+'.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1, bidirectional=True)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "c0 = Variable(torch.randn(2, 3, 20))\n",
    "output, hn = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0262 -0.0300  0.1385  0.0517 -0.1187 -0.1581  0.0058 -0.0630 -0.0708\n",
       "   0.0431  0.0000 -0.1865  0.2206  0.1467 -0.2882 -0.0504 -0.3230 -0.0736\n",
       "   0.1322 -0.0603  0.1803  0.1033 -0.2160 -0.1069 -0.0303  0.1110  0.0982\n",
       " \n",
       " Columns 9 to 17 \n",
       "    0.1379 -0.0159 -0.0569  0.1671 -0.1168  0.1358 -0.0823  0.0798  0.0735\n",
       "   0.1638  0.1187 -0.0314 -0.1135 -0.1871  0.0551  0.1918 -0.1770 -0.1505\n",
       "  -0.0077  0.1058 -0.0803  0.1280 -0.0647 -0.0149  0.1065 -0.1494  0.1058\n",
       " \n",
       " Columns 18 to 19 \n",
       "    0.0836 -0.0075\n",
       "  -0.0716 -0.1395\n",
       "   0.1533  0.0331\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0218  0.1405  0.0528  0.0870  0.2355  0.0391  0.1179 -0.1470  0.1026\n",
       "  -0.1145 -0.0303 -0.1846 -0.0444 -0.2343  0.0695  0.0592 -0.0462  0.0413\n",
       "   0.1245  0.1497  0.0318 -0.0747  0.1270  0.1806 -0.0454 -0.1718  0.1462\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.0491  0.0722  0.0430  0.1969 -0.0574 -0.0376  0.1289 -0.0570  0.0927\n",
       "  -0.0145  0.0295 -0.0678  0.0056  0.1946  0.1184  0.1774 -0.1404 -0.0458\n",
       "  -0.1120 -0.0548 -0.0249  0.1601  0.1792  0.1098 -0.2562  0.0462 -0.0287\n",
       " \n",
       " Columns 18 to 19 \n",
       "   -0.1299  0.0748\n",
       "  -0.0381 -0.2646\n",
       "  -0.2266 -0.0379\n",
       " [torch.FloatTensor of size 2x3x20], Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0727 -0.0482  0.2978  0.0970 -0.2090 -0.3533  0.0137 -0.1320 -0.1972\n",
       "   0.0912  0.0000 -0.3299  0.5086  0.2623 -0.4963 -0.1592 -0.5498 -0.1705\n",
       "   0.4063 -0.1658  0.3195  0.2035 -0.4086 -0.2152 -0.0672  0.2179  0.2869\n",
       " \n",
       " Columns 9 to 17 \n",
       "    0.2457 -0.0312 -0.0978  0.2423 -0.2201  0.2292 -0.1269  0.3032  0.1604\n",
       "   0.3014  0.2765 -0.0835 -0.1674 -0.3090  0.1504  0.3951 -0.3920 -0.3827\n",
       "  -0.0155  0.3424 -0.2246  0.2083 -0.1162 -0.0298  0.2375 -0.3460  0.1651\n",
       " \n",
       " Columns 18 to 19 \n",
       "    0.1906 -0.0168\n",
       "  -0.1533 -0.2548\n",
       "   0.3252  0.1103\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0631  0.2817  0.1168  0.2178  0.3444  0.0571  0.2109 -0.2296  0.2696\n",
       "  -0.1884 -0.0529 -0.3766 -0.0668 -0.3665  0.1372  0.1446 -0.0888  0.1378\n",
       "   0.2853  0.3587  0.0653 -0.1531  0.2440  0.3026 -0.0799 -0.3610  0.3513\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.1209  0.1599  0.1192  0.3646 -0.0949 -0.0617  0.2403 -0.2089  0.1627\n",
       "  -0.0267  0.0678 -0.1334  0.0103  0.3997  0.2439  0.3168 -0.2974 -0.0745\n",
       "  -0.2430 -0.1239 -0.0604  0.3928  0.3393  0.2123 -0.5639  0.1221 -0.0612\n",
       " \n",
       " Columns 18 to 19 \n",
       "   -0.2121  0.1477\n",
       "  -0.0695 -0.3737\n",
       "  -0.4132 -0.0618\n",
       " [torch.FloatTensor of size 2x3x20])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
