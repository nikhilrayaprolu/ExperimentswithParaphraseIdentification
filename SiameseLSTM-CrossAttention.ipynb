{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "def test(model, args, data, mode='test'):\n",
    "    if mode == 'dev':\n",
    "        iterator = iter(data.dev_iter)\n",
    "    else:\n",
    "        iterator = iter(data.test_iter)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    acc, loss, size = 0, 0, 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda(args.gpu)\n",
    "                char_h = char_h.cuda(args.gpu)\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = model(**kwargs)\n",
    "        pred = pred.view(-1,2)\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "\n",
    "        _, pred = pred.max(dim=1)\n",
    "        acc += (pred == batch.label).sum().float()\n",
    "        size += len(pred)\n",
    "\n",
    "    acc /= size\n",
    "    acc = acc.cpu().data[0]\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def load_model(args, data):\n",
    "    model = BIMPM(args, data)\n",
    "    model.load_state_dict(torch.load(args.model_path))\n",
    "\n",
    "    if args.gpu > -1:\n",
    "        model.cuda(args.gpu)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    Examples::\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_out = nn.Linear(dim*2, dim)\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, output, context):\n",
    "        batch_size = output.size(0)\n",
    "        hidden_size = output.size(2)\n",
    "        input_size = context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, input_size)).view(batch_size, -1, input_size)\n",
    "\n",
    "        # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        mix = torch.bmm(attn, context)\n",
    "\n",
    "        # concat -> (batch, out_len, 2*dim)\n",
    "        combined = torch.cat((mix, output), dim=2)\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = F.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return output, attn\n",
    "    \n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, args, data, use_attention = False):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n",
    "        self.l = self.args.num_perspective\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n",
    "\n",
    "        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        # no fine-tuning for word vectors\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        self.trainingtype = args.training\n",
    "        self.use_attention = use_attention\n",
    "        if self.use_attention:\n",
    "            self.attention = Attention(self.args.hidden_size*2)\n",
    "        \n",
    "        self.char_LSTM = nn.LSTM(\n",
    "            input_size=self.args.char_dim,\n",
    "            hidden_size=self.args.char_hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        self.context_LSTM = nn.LSTM(\n",
    "            input_size=self.d,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 4, self.args.hidden_size * 2)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 2, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Word Representation Layer -----\n",
    "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
    "        # zero vectors for padding\n",
    "        self.char_emb.weight.data[0].fill_(0)\n",
    "\n",
    "        # <unk> vectors is randomly initialized\n",
    "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
    "\n",
    "        nn.init.kaiming_normal(self.char_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.char_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0_reverse, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0_reverse, val=0)\n",
    "\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        p = self.word_emb(kwargs['p'])\n",
    "        h = self.word_emb(kwargs['h'])\n",
    "\n",
    "        if self.args.use_char_emb:\n",
    "            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n",
    "            seq_len_p = kwargs['char_p'].size(1)\n",
    "            seq_len_h = kwargs['char_h'].size(1)\n",
    "\n",
    "            char_p = kwargs['char_p'].view(-1, self.args.max_word_len)\n",
    "            char_h = kwargs['char_h'].view(-1, self.args.max_word_len)\n",
    "\n",
    "            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n",
    "            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n",
    "            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n",
    "\n",
    "            # (batch, seq_len, char_hidden_size)\n",
    "            char_p = char_p.view(-1, seq_len_p, self.args.char_hidden_size)\n",
    "            char_h = char_h.view(-1, seq_len_h, self.args.char_hidden_size)\n",
    "\n",
    "            # (batch, seq_len, word_dim + char_hidden_size)\n",
    "            p = torch.cat([p, char_p], dim=-1)\n",
    "            h = torch.cat([h, char_h], dim=-1)\n",
    "\n",
    "        p = self.dropout(p)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        # (batch, seq_len, hidden_size * 2)\n",
    "        #self.context_LSTM.flatten_parameters()\n",
    "        con_p, _ = self.context_LSTM(p)\n",
    "        con_h, _ = self.context_LSTM(h)\n",
    "        \n",
    "        #print(con_p.shape)\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        #print(con_p_fw.shape)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "        \n",
    "        con_p = self.dropout(con_p)\n",
    "        con_h = self.dropout(con_h)\n",
    "        \n",
    "        p_key = torch.cat([con_p_fw[:,-1,:],con_p_bw[:,0,:]], dim=-1)\n",
    "        h_key = torch.cat([con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        #print(h_key.shape, con_p.shape, con_h_fw.shape,con_h_fw[:,-1,:].shape, )\n",
    "        p_attn_output, attn_p = self.attention(h_key.view(-1,1,h_key.shape[1]), con_p)\n",
    "        \n",
    "        h_attn_output, attn_h = self.attention(p_key.view(-1,1,p_key.shape[1]), con_h)\n",
    "        #print(p_attn_output.shape)\n",
    "        x = torch.cat([p_attn_output, h_attn_output], dim=-1)\n",
    "        \n",
    "        #print(con_p_fw[:,-1,:].shape)\n",
    "        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n",
    "        #x = torch.cat(\n",
    "        #    [con_p_fw[:,-1,:],con_p_bw[:,0,:],con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading Quora data...\n",
      "training start!\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "foo:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 285.04415157437325 / dev loss: 42.460183292627335/ test loss:42.42969123274088 / dev acc:0.726099967956543test acc:0.7268999814987183\n",
      "train loss: 254.81220045685768 / dev loss: 44.41433426737785/ test loss:44.54421600699425 / dev acc:0.7170999646186829test acc:0.7094999551773071\n",
      "train loss: 241.21508634090424 / dev loss: 41.2238547205925/ test loss:41.022957026958466 / dev acc:0.7357999682426453test acc:0.7292999625205994\n",
      "train loss: 233.2641562372446 / dev loss: 37.574686989188194/ test loss:37.368300557136536 / dev acc:0.7762999534606934test acc:0.7684999704360962\n",
      "train loss: 228.13838713616133 / dev loss: 36.291295528411865/ test loss:36.481511786580086 / dev acc:0.7773000001907349test acc:0.7721999883651733\n",
      "train loss: 223.337473385036 / dev loss: 37.68466639518738/ test loss:37.897127620875835 / dev acc:0.7680000066757202test acc:0.7603999972343445\n",
      "epoch: 2\n",
      "train loss: 211.12327370792627 / dev loss: 34.6107729524374/ test loss:34.33119984343648 / dev acc:0.7944999933242798test acc:0.7895999550819397\n",
      "train loss: 210.1345073580742 / dev loss: 35.473241060972214/ test loss:35.31431940943003 / dev acc:0.7803999781608582test acc:0.7764999866485596\n",
      "train loss: 208.93457102775574 / dev loss: 34.06754283607006/ test loss:34.16072154790163 / dev acc:0.8007000088691711test acc:0.7910999655723572\n",
      "train loss: 203.60632829368114 / dev loss: 33.624660298228264/ test loss:33.287682581692934 / dev acc:0.7981999516487122test acc:0.7954999804496765\n",
      "train loss: 203.26868855953217 / dev loss: 34.52756616473198/ test loss:34.57556379586458 / dev acc:0.7896999716758728test acc:0.7888999581336975\n",
      "train loss: 201.19358100369573 / dev loss: 32.75244629383087/ test loss:32.710034523159266 / dev acc:0.8091999888420105test acc:0.8034999966621399\n",
      "epoch: 3\n",
      "train loss: 187.7955462858081 / dev loss: 33.28192789852619/ test loss:33.41483424603939 / dev acc:0.8078999519348145test acc:0.8021999597549438\n",
      "train loss: 190.1561875194311 / dev loss: 32.47877061367035/ test loss:32.66316078603268 / dev acc:0.8141999840736389test acc:0.8078999519348145\n",
      "train loss: 188.3222217783332 / dev loss: 34.40051190555096/ test loss:34.32292989268899 / dev acc:0.7994999885559082test acc:0.7932999730110168\n",
      "train loss: 187.66141864657402 / dev loss: 31.921328872442245/ test loss:32.43501702696085 / dev acc:0.8141999840736389test acc:0.807699978351593\n",
      "train loss: 185.83931957930326 / dev loss: 33.63309931755066/ test loss:33.996088802814484 / dev acc:0.8030999898910522test acc:0.7981999516487122\n",
      "train loss: 187.25848361849785 / dev loss: 33.31184211373329/ test loss:33.168283469974995 / dev acc:0.802899956703186test acc:0.8029999732971191\n",
      "epoch: 4\n",
      "train loss: 173.68991369754076 / dev loss: 31.463112488389015/ test loss:31.558162428438663 / dev acc:0.8176999688148499test acc:0.8154999613761902\n",
      "train loss: 171.81973776966333 / dev loss: 33.37424963712692/ test loss:33.31789703667164 / dev acc:0.8133999705314636test acc:0.8086000084877014\n",
      "train loss: 176.1201509013772 / dev loss: 31.575254395604134/ test loss:31.460369497537613 / dev acc:0.821399986743927test acc:0.8175999522209167\n",
      "train loss: 174.03060315549374 / dev loss: 31.737805232405663/ test loss:32.05953552573919 / dev acc:0.8133999705314636test acc:0.8114999532699585\n",
      "train loss: 175.4312224946916 / dev loss: 31.170426934957504/ test loss:31.167514756321907 / dev acc:0.821899950504303test acc:0.821399986743927\n",
      "train loss: 171.0998233705759 / dev loss: 30.30442564189434/ test loss:30.404234893620014 / dev acc:0.8270999789237976test acc:0.8220999836921692\n",
      "epoch: 5\n",
      "train loss: 159.9707864113152 / dev loss: 32.375636264681816/ test loss:32.1087062433362 / dev acc:0.8134999871253967test acc:0.8134999871253967\n",
      "train loss: 161.5746611431241 / dev loss: 30.55153926461935/ test loss:30.730074241757393 / dev acc:0.8259999752044678test acc:0.8251000046730042\n",
      "train loss: 165.49186441674829 / dev loss: 30.536867558956146/ test loss:30.368486378341913 / dev acc:0.8307999968528748test acc:0.8303999900817871\n",
      "train loss: 161.69419076293707 / dev loss: 32.36792466044426/ test loss:32.17898225039244 / dev acc:0.814799964427948test acc:0.8170999884605408\n",
      "train loss: 163.22479212284088 / dev loss: 30.789234906435013/ test loss:30.57427554577589 / dev acc:0.8289999961853027test acc:0.8289999961853027\n",
      "train loss: 163.90524334833026 / dev loss: 31.992172330617905/ test loss:31.91361577808857 / dev acc:0.8217999935150146test acc:0.8209999799728394\n",
      "epoch: 6\n",
      "train loss: 149.4801534563303 / dev loss: 30.874462455511093/ test loss:31.14628767967224 / dev acc:0.8270999789237976test acc:0.8278999924659729\n",
      "train loss: 149.83542524278164 / dev loss: 31.33500124514103/ test loss:31.186504043638706 / dev acc:0.8281999826431274test acc:0.8310999870300293\n",
      "train loss: 152.47014647349715 / dev loss: 31.834530889987946/ test loss:31.730421729385853 / dev acc:0.8260999917984009test acc:0.8248999714851379\n",
      "train loss: 152.90792544186115 / dev loss: 30.558882862329483/ test loss:30.132688842713833 / dev acc:0.8273999691009521test acc:0.8317999839782715\n",
      "train loss: 155.2441536821425 / dev loss: 29.498797342181206/ test loss:29.637217976152897 / dev acc:0.8353999853134155test acc:0.8353999853134155\n",
      "train loss: 153.34130969643593 / dev loss: 31.482351511716843/ test loss:31.437216944992542 / dev acc:0.8248999714851379test acc:0.8260999917984009\n",
      "epoch: 7\n",
      "train loss: 141.82486698776484 / dev loss: 30.00964444875717/ test loss:29.810786426067352 / dev acc:0.8333999514579773test acc:0.835099995136261\n",
      "train loss: 139.7569370251149 / dev loss: 30.118614077568054/ test loss:29.92190419510007 / dev acc:0.8370999693870544test acc:0.8378999829292297\n",
      "train loss: 144.2881275769323 / dev loss: 29.593430012464523/ test loss:29.758343309164047 / dev acc:0.8352999687194824test acc:0.8342999815940857\n",
      "train loss: 143.57065105438232 / dev loss: 29.997181549668312/ test loss:30.235370252281427 / dev acc:0.8317999839782715test acc:0.8305000066757202\n",
      "train loss: 146.45463190414011 / dev loss: 30.36479477584362/ test loss:30.45053930580616 / dev acc:0.8312999606132507test acc:0.8313999772071838\n",
      "train loss: 148.0571212656796 / dev loss: 28.617006108164787/ test loss:29.091356247663498 / dev acc:0.8388999700546265test acc:0.8360999822616577\n",
      "epoch: 8\n",
      "train loss: 129.45154886692762 / dev loss: 31.365088306367397/ test loss:32.249342285096645 / dev acc:0.8319999575614929test acc:0.8323000073432922\n",
      "train loss: 135.05487003922462 / dev loss: 31.353450745344162/ test loss:31.475306034088135 / dev acc:0.8270999789237976test acc:0.8300999999046326\n",
      "train loss: 136.223758591339 / dev loss: 30.806293606758118/ test loss:31.297880068421364 / dev acc:0.8258999586105347test acc:0.8228999972343445\n",
      "train loss: 138.46718871966004 / dev loss: 29.661595091223717/ test loss:29.984761588275433 / dev acc:0.8363999724388123test acc:0.8317999839782715\n",
      "train loss: 135.92326662503183 / dev loss: 29.70809619128704/ test loss:29.832384824752808 / dev acc:0.8407999873161316test acc:0.8385999798774719\n",
      "train loss: 140.33559065312147 / dev loss: 28.55523670464754/ test loss:28.91966027021408 / dev acc:0.8399999737739563test acc:0.8396999835968018\n",
      "epoch: 9\n",
      "train loss: 126.02195870690048 / dev loss: 30.707619234919548/ test loss:31.000543903559446 / dev acc:0.8305999636650085test acc:0.8288999795913696\n",
      "train loss: 124.54977561719716 / dev loss: 30.687066063284874/ test loss:30.26197125017643 / dev acc:0.8357999920845032test acc:0.8361999988555908\n",
      "train loss: 130.06830547563732 / dev loss: 30.772745840251446/ test loss:30.684117913246155 / dev acc:0.8359000086784363test acc:0.837399959564209\n",
      "train loss: 131.39445855095983 / dev loss: 30.738252632319927/ test loss:30.68965946137905 / dev acc:0.8359000086784363test acc:0.8378999829292297\n",
      "train loss: 131.6803505383432 / dev loss: 30.52521502971649/ test loss:30.211754769086838 / dev acc:0.837399959564209test acc:0.8384000062942505\n",
      "train loss: 134.66780226305127 / dev loss: 30.062647327780724/ test loss:30.043369121849537 / dev acc:0.835599958896637test acc:0.8324999809265137\n",
      "epoch: 10\n",
      "train loss: 117.17114401981235 / dev loss: 30.576726749539375/ test loss:30.45272096991539 / dev acc:0.8313999772071838test acc:0.8296999931335449\n",
      "train loss: 122.98263919260353 / dev loss: 31.62022352963686/ test loss:31.276900246739388 / dev acc:0.8352999687194824test acc:0.8315999507904053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 122.49967833049595 / dev loss: 29.783070370554924/ test loss:29.730550423264503 / dev acc:0.8424999713897705test acc:0.8414999842643738\n",
      "train loss: 126.19971011765301 / dev loss: 30.357310742139816/ test loss:30.39806454628706 / dev acc:0.8377999663352966test acc:0.8379999995231628\n",
      "train loss: 127.23563053831458 / dev loss: 29.71496543288231/ test loss:29.77551382035017 / dev acc:0.8418999910354614test acc:0.836899995803833\n",
      "train loss: 125.69811818748713 / dev loss: 30.74360364675522/ test loss:31.28840759396553 / dev acc:0.8355000019073486test acc:0.8345999717712402\n",
      "epoch: 11\n",
      "train loss: 114.84744810499251 / dev loss: 30.45626801252365/ test loss:30.774033211171627 / dev acc:0.8399999737739563test acc:0.8375999927520752\n",
      "train loss: 114.9146527890116 / dev loss: 30.505833730101585/ test loss:31.18884986639023 / dev acc:0.8442999720573425test acc:0.8391000032424927\n",
      "train loss: 119.28968821652234 / dev loss: 29.35479548573494/ test loss:29.571572177112103 / dev acc:0.8378999829292297test acc:0.838699996471405\n",
      "train loss: 121.06878765672445 / dev loss: 30.09298711270094/ test loss:30.78192124143243 / dev acc:0.839199960231781test acc:0.8362999558448792\n",
      "train loss: 122.09509890526533 / dev loss: 30.886774841696024/ test loss:31.240081071853638 / dev acc:0.8434000015258789test acc:0.8393999934196472\n",
      "train loss: 121.29788428917527 / dev loss: 29.96316310763359/ test loss:30.283038169145584 / dev acc:0.8435999751091003test acc:0.8414999842643738\n",
      "epoch: 12\n",
      "train loss: 109.5980400685221 / dev loss: 33.32004765421152/ test loss:34.230412140488625 / dev acc:0.8330999612808228test acc:0.8313999772071838\n",
      "train loss: 112.72627487406135 / dev loss: 30.896001160144806/ test loss:30.864027455449104 / dev acc:0.8431999683380127test acc:0.8402000069618225\n",
      "train loss: 111.77466640248895 / dev loss: 30.355373941361904/ test loss:30.641254037618637 / dev acc:0.8417999744415283test acc:0.8374999761581421\n",
      "train loss: 117.95483653247356 / dev loss: 30.472560986876488/ test loss:31.19540622830391 / dev acc:0.8396999835968018test acc:0.8399999737739563\n",
      "train loss: 117.74182371795177 / dev loss: 30.81433743983507/ test loss:31.19147650152445 / dev acc:0.8436999917030334test acc:0.8446999788284302\n",
      "train loss: 118.6573295975104 / dev loss: 31.706524096429348/ test loss:32.478250689804554 / dev acc:0.8369999527931213test acc:0.8330000042915344\n",
      "epoch: 13\n",
      "train loss: 105.2593097742647 / dev loss: 31.992999382317066/ test loss:32.29446278512478 / dev acc:0.8375999927520752test acc:0.8378999829292297\n",
      "train loss: 108.22788172960281 / dev loss: 32.12376298755407/ test loss:32.16319401562214 / dev acc:0.8424999713897705test acc:0.8402999639511108\n",
      "train loss: 109.32072086632252 / dev loss: 31.275935292243958/ test loss:31.393655188381672 / dev acc:0.8427000045776367test acc:0.8427000045776367\n",
      "train loss: 113.70559903793037 / dev loss: 31.917995184659958/ test loss:32.42699637264013 / dev acc:0.8448999524116516test acc:0.8402999639511108\n",
      "train loss: 115.12686142325401 / dev loss: 31.75089342147112/ test loss:32.20803210884333 / dev acc:0.8428999781608582test acc:0.8385999798774719\n",
      "train loss: 112.30669243261218 / dev loss: 31.02938763797283/ test loss:31.70406600832939 / dev acc:0.838699996471405test acc:0.8374999761581421\n",
      "epoch: 14\n",
      "train loss: 99.62265968788415 / dev loss: 32.37386406958103/ test loss:32.830320566892624 / dev acc:0.8398000001907349test acc:0.8367999792098999\n",
      "train loss: 104.21242612227798 / dev loss: 32.46203303337097/ test loss:34.0217676833272 / dev acc:0.8425999879837036test acc:0.8382999897003174\n",
      "train loss: 106.52731825783849 / dev loss: 31.540320873260498/ test loss:32.466749504208565 / dev acc:0.8398000001907349test acc:0.8396999835968018\n",
      "train loss: 108.97860287874937 / dev loss: 31.528697185218334/ test loss:32.22628343105316 / dev acc:0.8418999910354614test acc:0.8373000025749207\n",
      "train loss: 110.32957923412323 / dev loss: 32.19272741675377/ test loss:33.4163341447711 / dev acc:0.8445000052452087test acc:0.8382999897003174\n",
      "train loss: 113.26799206621945 / dev loss: 29.480371810495853/ test loss:30.26859126240015 / dev acc:0.8452000021934509test acc:0.8405999541282654\n",
      "epoch: 15\n",
      "train loss: 97.16227377578616 / dev loss: 32.52934990823269/ test loss:33.024943210184574 / dev acc:0.8428999781608582test acc:0.8384999632835388\n",
      "train loss: 101.18422036059201 / dev loss: 34.43565025180578/ test loss:34.193423718214035 / dev acc:0.840399980545044test acc:0.8399999737739563\n",
      "train loss: 104.8029772657901 / dev loss: 32.13865301012993/ test loss:32.84827692806721 / dev acc:0.8400999903678894test acc:0.8359999656677246\n",
      "train loss: 106.37118091247976 / dev loss: 31.32232780009508/ test loss:32.11643299460411 / dev acc:0.8402999639511108test acc:0.8349999785423279\n",
      "train loss: 104.44570080656558 / dev loss: 31.701792135834694/ test loss:31.96564318984747 / dev acc:0.835099995136261test acc:0.8388999700546265\n",
      "train loss: 109.38453091308475 / dev loss: 30.25247387588024/ test loss:30.681560944765806 / dev acc:0.8398000001907349test acc:0.8414999842643738\n",
      "max dev acc:0.8452000021934509/ max test acc: 0.8405999541282654\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'int' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9ce84a9e1f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-9ce84a9e1f30>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/BIBPM_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'int' object to str implicitly"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = (Siamese(args, data,use_attention = True))\n",
    "    if args.gpu > -1:\n",
    "        model.cuda(args.gpu)\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc = 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', str(present_epoch + 1))\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "\n",
    "        # limit the lengths of input sentences up to max_sent_len\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda(args.gpu)\n",
    "                char_h = char_h.cuda(args.gpu)\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = (model(**kwargs))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(pred.shape, batch.label.shape)\n",
    "        batch_loss = criterion(pred.view(-1,2), batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        del pred\n",
    "        del batch_loss\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, args, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print('train loss: '+ str(loss) +' / dev loss: '+ str(dev_loss) + '/ test loss:' + str(test_loss) +\n",
    "                  ' / dev acc:' + str(dev_acc) + 'test acc:' + str(test_acc))\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print('max dev acc:'+ str(max_dev_acc) + '/ max test acc: ' + str(max_test_acc))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=128, type=int)\n",
    "    parser.add_argument('--char-dim', default=20, type=int)\n",
    "    parser.add_argument('--char-hidden-size', default=50, type=int)\n",
    "    parser.add_argument('--data-type', default='Quora', help='available: SNLI or Quora')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=15, type=int)\n",
    "    parser.add_argument('--gpu', default=1, type=int)\n",
    "    parser.add_argument('--hidden-size', default=100, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.001, type=float)\n",
    "    parser.add_argument('--max-sent-len', default=-1, type=int,\n",
    "                        help='max length of input sentences model can accept, if -1, it accepts any length')\n",
    "    parser.add_argument('--num-perspective', default=20, type=int)\n",
    "    parser.add_argument('--print-freq', default=500, type=int)\n",
    "    parser.add_argument('--use-char-emb', default=False, action='store_true')\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "    parser.add_argument('--training', default=0, type=int)\n",
    "    args = parser.parse_args()\n",
    "    print(args.training)\n",
    "    if args.data_type == 'SNLI':\n",
    "        print('loading SNLI data...')\n",
    "        data = SNLI(args)\n",
    "    elif args.data_type == 'Quora':\n",
    "        print('loading Quora data...')\n",
    "        data = Quora(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
    "\n",
    "    setattr(args, 'char_vocab_size', len(data.char_vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'max_word_len', data.max_word_len)\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), 'saved_models/BIBPM_'+args.data_type+'_'+args.model_time+'train'+args.training+'.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1, bidirectional=True)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "c0 = Variable(torch.randn(2, 3, 20))\n",
    "output, hn = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0262 -0.0300  0.1385  0.0517 -0.1187 -0.1581  0.0058 -0.0630 -0.0708\n",
       "   0.0431  0.0000 -0.1865  0.2206  0.1467 -0.2882 -0.0504 -0.3230 -0.0736\n",
       "   0.1322 -0.0603  0.1803  0.1033 -0.2160 -0.1069 -0.0303  0.1110  0.0982\n",
       " \n",
       " Columns 9 to 17 \n",
       "    0.1379 -0.0159 -0.0569  0.1671 -0.1168  0.1358 -0.0823  0.0798  0.0735\n",
       "   0.1638  0.1187 -0.0314 -0.1135 -0.1871  0.0551  0.1918 -0.1770 -0.1505\n",
       "  -0.0077  0.1058 -0.0803  0.1280 -0.0647 -0.0149  0.1065 -0.1494  0.1058\n",
       " \n",
       " Columns 18 to 19 \n",
       "    0.0836 -0.0075\n",
       "  -0.0716 -0.1395\n",
       "   0.1533  0.0331\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0218  0.1405  0.0528  0.0870  0.2355  0.0391  0.1179 -0.1470  0.1026\n",
       "  -0.1145 -0.0303 -0.1846 -0.0444 -0.2343  0.0695  0.0592 -0.0462  0.0413\n",
       "   0.1245  0.1497  0.0318 -0.0747  0.1270  0.1806 -0.0454 -0.1718  0.1462\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.0491  0.0722  0.0430  0.1969 -0.0574 -0.0376  0.1289 -0.0570  0.0927\n",
       "  -0.0145  0.0295 -0.0678  0.0056  0.1946  0.1184  0.1774 -0.1404 -0.0458\n",
       "  -0.1120 -0.0548 -0.0249  0.1601  0.1792  0.1098 -0.2562  0.0462 -0.0287\n",
       " \n",
       " Columns 18 to 19 \n",
       "   -0.1299  0.0748\n",
       "  -0.0381 -0.2646\n",
       "  -0.2266 -0.0379\n",
       " [torch.FloatTensor of size 2x3x20], Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0727 -0.0482  0.2978  0.0970 -0.2090 -0.3533  0.0137 -0.1320 -0.1972\n",
       "   0.0912  0.0000 -0.3299  0.5086  0.2623 -0.4963 -0.1592 -0.5498 -0.1705\n",
       "   0.4063 -0.1658  0.3195  0.2035 -0.4086 -0.2152 -0.0672  0.2179  0.2869\n",
       " \n",
       " Columns 9 to 17 \n",
       "    0.2457 -0.0312 -0.0978  0.2423 -0.2201  0.2292 -0.1269  0.3032  0.1604\n",
       "   0.3014  0.2765 -0.0835 -0.1674 -0.3090  0.1504  0.3951 -0.3920 -0.3827\n",
       "  -0.0155  0.3424 -0.2246  0.2083 -0.1162 -0.0298  0.2375 -0.3460  0.1651\n",
       " \n",
       " Columns 18 to 19 \n",
       "    0.1906 -0.0168\n",
       "  -0.1533 -0.2548\n",
       "   0.3252  0.1103\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0631  0.2817  0.1168  0.2178  0.3444  0.0571  0.2109 -0.2296  0.2696\n",
       "  -0.1884 -0.0529 -0.3766 -0.0668 -0.3665  0.1372  0.1446 -0.0888  0.1378\n",
       "   0.2853  0.3587  0.0653 -0.1531  0.2440  0.3026 -0.0799 -0.3610  0.3513\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.1209  0.1599  0.1192  0.3646 -0.0949 -0.0617  0.2403 -0.2089  0.1627\n",
       "  -0.0267  0.0678 -0.1334  0.0103  0.3997  0.2439  0.3168 -0.2974 -0.0745\n",
       "  -0.2430 -0.1239 -0.0604  0.3928  0.3393  0.2123 -0.5639  0.1221 -0.0612\n",
       " \n",
       " Columns 18 to 19 \n",
       "   -0.2121  0.1477\n",
       "  -0.0695 -0.3737\n",
       "  -0.4132 -0.0618\n",
       " [torch.FloatTensor of size 2x3x20])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
