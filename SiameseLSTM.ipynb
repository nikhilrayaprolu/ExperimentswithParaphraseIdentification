{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Siamese(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n",
    "        self.l = self.args.num_perspective\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n",
    "\n",
    "        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        # no fine-tuning for word vectors\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        self.trainingtype = args.training\n",
    "        self.char_LSTM = nn.LSTM(\n",
    "            input_size=self.args.char_dim,\n",
    "            hidden_size=self.args.char_hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        self.context_LSTM = nn.LSTM(\n",
    "            input_size=self.d,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 4, self.args.hidden_size * 2)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 2, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Word Representation Layer -----\n",
    "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
    "        # zero vectors for padding\n",
    "        self.char_emb.weight.data[0].fill_(0)\n",
    "\n",
    "        # <unk> vectors is randomly initialized\n",
    "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
    "\n",
    "        nn.init.kaiming_normal(self.char_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.char_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0_reverse, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0_reverse, val=0)\n",
    "\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        p = self.word_emb(kwargs['p'])\n",
    "        h = self.word_emb(kwargs['h'])\n",
    "\n",
    "        if self.args.use_char_emb:\n",
    "            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n",
    "            seq_len_p = kwargs['char_p'].size(1)\n",
    "            seq_len_h = kwargs['char_h'].size(1)\n",
    "\n",
    "            char_p = kwargs['char_p'].view(-1, self.args.max_word_len)\n",
    "            char_h = kwargs['char_h'].view(-1, self.args.max_word_len)\n",
    "\n",
    "            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n",
    "            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n",
    "            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n",
    "\n",
    "            # (batch, seq_len, char_hidden_size)\n",
    "            char_p = char_p.view(-1, seq_len_p, self.args.char_hidden_size)\n",
    "            char_h = char_h.view(-1, seq_len_h, self.args.char_hidden_size)\n",
    "\n",
    "            # (batch, seq_len, word_dim + char_hidden_size)\n",
    "            p = torch.cat([p, char_p], dim=-1)\n",
    "            h = torch.cat([h, char_h], dim=-1)\n",
    "\n",
    "        p = self.dropout(p)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        # (batch, seq_len, hidden_size * 2)\n",
    "        #self.context_LSTM.flatten_parameters()\n",
    "        con_p, _ = self.context_LSTM(p)\n",
    "        con_h, _ = self.context_LSTM(h)\n",
    "        #print(con_p.shape)\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        #print(con_p_fw.shape)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "\n",
    "        con_p = self.dropout(con_p)\n",
    "        con_h = self.dropout(con_h)\n",
    "        #print(con_p_fw[:,-1,:].shape)\n",
    "        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n",
    "        x = torch.cat(\n",
    "            [con_p_fw[:,-1,:],con_p_bw[:,0,:],con_h_fw[:,-1,:],con_h_bw[:,0,:]], dim=-1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loading Quora data...\n",
      "training start!\n",
      "epoch: 1\n",
      "train loss: 294.4184825718403 / dev loss: 49.21399363875389/ test loss:49.52327512949705 / dev acc:0.6708999872207642test acc:0.6703999638557434\n",
      "train loss: 271.91084460914135 / dev loss: 46.47843912243843/ test loss:46.25057019293308 / dev acc:0.6901999711990356test acc:0.6942999958992004\n",
      "train loss: 262.7715002298355 / dev loss: 44.96004927158356/ test loss:44.83750591427088 / dev acc:0.7053999900817871test acc:0.7096999883651733\n",
      "train loss: 257.15390276908875 / dev loss: 44.9849448800087/ test loss:44.55301072448492 / dev acc:0.7076999545097351test acc:0.7075999975204468\n",
      "train loss: 255.72855180501938 / dev loss: 42.68616831302643/ test loss:42.76470547914505 / dev acc:0.7311999797821045test acc:0.7290999889373779\n",
      "train loss: 249.16862561553717 / dev loss: 43.63723450899124/ test loss:43.500356167554855 / dev acc:0.7131999731063843test acc:0.7105000019073486\n",
      "epoch: 2\n",
      "train loss: 244.55566601455212 / dev loss: 41.25997471809387/ test loss:41.16963084042072 / dev acc:0.7386999726295471test acc:0.7350999712944031\n",
      "train loss: 242.3482587262988 / dev loss: 40.817056238651276/ test loss:40.556938625872135 / dev acc:0.7440999746322632test acc:0.739799976348877\n",
      "train loss: 240.1543180719018 / dev loss: 43.34243842959404/ test loss:43.05660334229469 / dev acc:0.7353999614715576test acc:0.731499969959259\n",
      "train loss: 239.38182258605957 / dev loss: 40.74207058548927/ test loss:40.81512585282326 / dev acc:0.7389999628067017test acc:0.729200005531311\n",
      "train loss: 234.4114998653531 / dev loss: 40.4136560857296/ test loss:40.163486398756504 / dev acc:0.7562999725341797test acc:0.7519999742507935\n",
      "train loss: 232.93954546004534 / dev loss: 39.216202199459076/ test loss:38.92897206544876 / dev acc:0.7605999708175659test acc:0.7587999701499939\n",
      "epoch: 3\n",
      "train loss: 223.1227429136634 / dev loss: 43.203663885593414/ test loss:43.28198300302029 / dev acc:0.726099967956543test acc:0.7246999740600586\n",
      "train loss: 223.14999283105135 / dev loss: 39.44513610005379/ test loss:39.499814473092556 / dev acc:0.7529999613761902test acc:0.7509999871253967\n",
      "train loss: 222.26267801225185 / dev loss: 38.2999501824379/ test loss:38.15009921789169 / dev acc:0.7594000101089478test acc:0.758899986743927\n",
      "train loss: 219.77666997164488 / dev loss: 38.634018272161484/ test loss:38.24289098381996 / dev acc:0.765999972820282test acc:0.7622999548912048\n",
      "train loss: 218.85733928531408 / dev loss: 40.41494345664978/ test loss:39.68702454864979 / dev acc:0.7430999875068665test acc:0.7462999820709229\n",
      "train loss: 216.7098501548171 / dev loss: 37.460029274225235/ test loss:37.01787509769201 / dev acc:0.7691999673843384test acc:0.7675999999046326\n",
      "epoch: 4\n",
      "train loss: 206.66991069167852 / dev loss: 39.540851414203644/ test loss:39.152917854487896 / dev acc:0.7543999552726746test acc:0.7562999725341797\n",
      "train loss: 205.38912816345692 / dev loss: 37.95813684165478/ test loss:37.27660634741187 / dev acc:0.7665999531745911test acc:0.7716000080108643\n",
      "train loss: 207.66740074008703 / dev loss: 37.93408992886543/ test loss:37.420694313943386 / dev acc:0.7724999785423279test acc:0.7705000042915344\n",
      "train loss: 205.67698439210653 / dev loss: 36.9612640440464/ test loss:36.59681270644069 / dev acc:0.7716000080108643test acc:0.7710999846458435\n",
      "train loss: 206.27984727919102 / dev loss: 37.6088285446167/ test loss:37.336431577801704 / dev acc:0.7730000019073486test acc:0.7698999643325806\n",
      "train loss: 205.93567076325417 / dev loss: 36.700871765613556/ test loss:36.67410631477833 / dev acc:0.7749999761581421test acc:0.7734999656677246\n",
      "epoch: 5\n",
      "train loss: 193.85328106582165 / dev loss: 36.48313219845295/ test loss:36.37717701494694 / dev acc:0.7821999788284302test acc:0.7752999663352966\n",
      "train loss: 195.0873627960682 / dev loss: 35.57049985229969/ test loss:35.64337431639433 / dev acc:0.7870999574661255test acc:0.7749999761581421\n",
      "train loss: 193.39097391813993 / dev loss: 37.087022721767426/ test loss:37.09153953939676 / dev acc:0.7771999835968018test acc:0.7737999558448792\n",
      "train loss: 194.94002439826727 / dev loss: 37.07144093513489/ test loss:36.85795919969678 / dev acc:0.7780999541282654test acc:0.7663999795913696\n",
      "train loss: 196.18376448750496 / dev loss: 37.787073239684105/ test loss:37.1189504340291 / dev acc:0.7748000025749207test acc:0.7710999846458435\n",
      "train loss: 192.6472272053361 / dev loss: 36.304500102996826/ test loss:36.25926364958286 / dev acc:0.7807999849319458test acc:0.7766000032424927\n",
      "epoch: 6\n",
      "train loss: 181.65098226070404 / dev loss: 36.917442455887794/ test loss:37.20848228037357 / dev acc:0.7829999923706055test acc:0.7723999619483948\n",
      "train loss: 180.7372735887766 / dev loss: 35.82350170612335/ test loss:35.77377628535032 / dev acc:0.7904999852180481test acc:0.7818999886512756\n",
      "train loss: 183.9370876699686 / dev loss: 35.9320003092289/ test loss:35.832005113363266 / dev acc:0.7825999855995178test acc:0.7762999534606934\n",
      "train loss: 185.58652177080512 / dev loss: 35.05881376564503/ test loss:35.370780400931835 / dev acc:0.7967000007629395test acc:0.7877999544143677\n",
      "train loss: 182.89539283141494 / dev loss: 38.38751405477524/ test loss:38.27219135314226 / dev acc:0.7798999547958374test acc:0.7744999527931213\n",
      "train loss: 186.06284160912037 / dev loss: 34.30162088572979/ test loss:34.237556613981724 / dev acc:0.7954999804496765test acc:0.7950999736785889\n",
      "epoch: 7\n",
      "train loss: 170.8463343679905 / dev loss: 36.16529268026352/ test loss:36.49022734165192 / dev acc:0.7910999655723572test acc:0.7813999652862549\n",
      "train loss: 174.1747726276517 / dev loss: 35.84299097955227/ test loss:35.913555681705475 / dev acc:0.7924000024795532test acc:0.7890999913215637\n",
      "train loss: 175.88355649262667 / dev loss: 36.12339800596237/ test loss:36.156000189483166 / dev acc:0.7910999655723572test acc:0.7846999764442444\n",
      "train loss: 175.80359702557325 / dev loss: 34.5459955483675/ test loss:34.96155634894967 / dev acc:0.7967999577522278test acc:0.7876999974250793\n",
      "train loss: 177.36257571727037 / dev loss: 36.42065866291523/ test loss:36.7258655205369 / dev acc:0.7896999716758728test acc:0.780299961566925\n",
      "train loss: 176.81151984632015 / dev loss: 34.52618306875229/ test loss:34.83977423608303 / dev acc:0.7978000044822693test acc:0.7918999791145325\n",
      "epoch: 8\n",
      "train loss: 161.91745191812515 / dev loss: 36.723208963871/ test loss:36.775788210332394 / dev acc:0.7912999987602234test acc:0.7831999659538269\n",
      "train loss: 165.2732714265585 / dev loss: 36.097493916749954/ test loss:36.34949351102114 / dev acc:0.7888999581336975test acc:0.7892999649047852\n",
      "train loss: 166.65074311941862 / dev loss: 34.63117890059948/ test loss:34.81376029551029 / dev acc:0.79749995470047test acc:0.7918999791145325\n",
      "train loss: 169.41444181650877 / dev loss: 35.32538339495659/ test loss:35.38260744139552 / dev acc:0.7985000014305115test acc:0.788100004196167\n",
      "train loss: 169.8342183381319 / dev loss: 35.28198277950287/ test loss:35.81144060194492 / dev acc:0.7953000068664551test acc:0.7886999845504761\n",
      "train loss: 172.89814764261246 / dev loss: 35.55717431008816/ test loss:35.9030230268836 / dev acc:0.8032999634742737test acc:0.789900004863739\n",
      "epoch: 9\n",
      "train loss: 155.67138005048037 / dev loss: 35.11001127958298/ test loss:35.44843465089798 / dev acc:0.8064000010490417test acc:0.7929999828338623\n",
      "train loss: 157.18557766452432 / dev loss: 34.88826856017113/ test loss:34.699806589633226 / dev acc:0.8036999702453613test acc:0.7973999977111816\n",
      "train loss: 162.14498459547758 / dev loss: 34.73733927309513/ test loss:34.456052385270596 / dev acc:0.8030999898910522test acc:0.7991999983787537\n",
      "train loss: 162.2173659130931 / dev loss: 37.13375003635883/ test loss:36.92356237024069 / dev acc:0.7958999872207642test acc:0.7913999557495117\n",
      "train loss: 162.55316466093063 / dev loss: 37.700154691934586/ test loss:37.98461563885212 / dev acc:0.7932999730110168test acc:0.7836999893188477\n",
      "train loss: 163.44222632423043 / dev loss: 35.23804259300232/ test loss:35.31831517070532 / dev acc:0.8011999726295471test acc:0.7894999980926514\n",
      "epoch: 10\n",
      "train loss: 150.00533882901073 / dev loss: 37.00859126448631/ test loss:37.576818108558655 / dev acc:0.7967999577522278test acc:0.7899999618530273\n",
      "train loss: 148.69528206437826 / dev loss: 37.61406645178795/ test loss:38.29866545647383 / dev acc:0.7953999638557434test acc:0.7890999913215637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 157.01515647023916 / dev loss: 34.852437406778336/ test loss:34.98790846765041 / dev acc:0.8093999624252319test acc:0.7991999983787537\n",
      "train loss: 156.33865844458342 / dev loss: 34.412106826901436/ test loss:34.69887565821409 / dev acc:0.805899977684021test acc:0.7981999516487122\n",
      "train loss: 159.18577799759805 / dev loss: 36.38910350203514/ test loss:36.42680036276579 / dev acc:0.8061999678611755test acc:0.7964999675750732\n",
      "train loss: 158.72115605324507 / dev loss: 33.5940053910017/ test loss:34.11306235939264 / dev acc:0.809499979019165test acc:0.7965999841690063\n",
      "epoch: 11\n",
      "train loss: 143.65073038637638 / dev loss: 36.985296458005905/ test loss:38.1807572171092 / dev acc:0.805899977684021test acc:0.7910999655723572\n",
      "train loss: 147.9505419060588 / dev loss: 35.81538861989975/ test loss:36.34571927040815 / dev acc:0.805899977684021test acc:0.7978999614715576\n",
      "train loss: 148.86081743240356 / dev loss: 36.619073897600174/ test loss:36.94346824288368 / dev acc:0.8017999529838562test acc:0.7910000085830688\n",
      "train loss: 151.44329958036542 / dev loss: 36.571903705596924/ test loss:36.947933815419674 / dev acc:0.800599992275238test acc:0.7947999835014343\n",
      "train loss: 152.62195152789354 / dev loss: 36.01880140602589/ test loss:36.826607055962086 / dev acc:0.8047999739646912test acc:0.7929999828338623\n",
      "train loss: 154.56627476215363 / dev loss: 34.593147188425064/ test loss:34.88565118610859 / dev acc:0.8066999912261963test acc:0.7976999878883362\n",
      "epoch: 12\n",
      "train loss: 140.16224867478013 / dev loss: 38.45930352807045/ test loss:38.52583829127252 / dev acc:0.7971999645233154test acc:0.7929999828338623\n",
      "train loss: 142.62951678410172 / dev loss: 36.047102242708206/ test loss:36.37928806990385 / dev acc:0.8014000058174133test acc:0.7965999841690063\n",
      "train loss: 145.1286363862455 / dev loss: 36.91321936249733/ test loss:36.9613871127367 / dev acc:0.8057999610900879test acc:0.8007999658584595\n",
      "train loss: 146.8552581295371 / dev loss: 36.42299096286297/ test loss:36.61266727000475 / dev acc:0.8100000023841858test acc:0.7967000007629395\n",
      "train loss: 149.37325828894973 / dev loss: 35.45545011758804/ test loss:36.107442405074835 / dev acc:0.807699978351593test acc:0.7955999970436096\n",
      "train loss: 150.923644348979 / dev loss: 34.728212371468544/ test loss:35.500632628798485 / dev acc:0.8109999895095825test acc:0.7979999780654907\n",
      "epoch: 13\n",
      "train loss: 137.17184436321259 / dev loss: 36.111481577157974/ test loss:36.71986848860979 / dev acc:0.8068999648094177test acc:0.7928999662399292\n",
      "train loss: 139.0720729418099 / dev loss: 36.477851912379265/ test loss:36.79139004647732 / dev acc:0.8079999685287476test acc:0.7960000038146973\n",
      "train loss: 140.52503784373403 / dev loss: 36.807019874453545/ test loss:37.057725600898266 / dev acc:0.8108999729156494test acc:0.7991999983787537\n",
      "train loss: 139.8176471926272 / dev loss: 35.628813952207565/ test loss:36.2725117020309 / dev acc:0.809499979019165test acc:0.8017999529838562\n",
      "train loss: 144.23852620087564 / dev loss: 35.44999738037586/ test loss:35.77912733703852 / dev acc:0.8137999773025513test acc:0.8014000058174133\n",
      "train loss: 146.62500767409801 / dev loss: 35.269174203276634/ test loss:35.36633865535259 / dev acc:0.809499979019165test acc:0.8035999536514282\n",
      "epoch: 14\n",
      "train loss: 132.36165219917893 / dev loss: 37.588999912142754/ test loss:37.552317790687084 / dev acc:0.8075000047683716test acc:0.7997999787330627\n",
      "train loss: 136.32864057272673 / dev loss: 35.99162709712982/ test loss:36.42293683439493 / dev acc:0.8084999918937683test acc:0.7973999977111816\n",
      "train loss: 135.47369707003236 / dev loss: 36.518328443169594/ test loss:36.698889166116714 / dev acc:0.8125999569892883test acc:0.8032000064849854\n",
      "train loss: 138.63596818223596 / dev loss: 36.86207889020443/ test loss:37.36918997019529 / dev acc:0.8079999685287476test acc:0.8021999597549438\n",
      "train loss: 140.54519291594625 / dev loss: 36.3028019964695/ test loss:36.96740975230932 / dev acc:0.8053999543190002test acc:0.795199990272522\n",
      "train loss: 140.77608006820083 / dev loss: 35.60682101547718/ test loss:36.4158254340291 / dev acc:0.8095999956130981test acc:0.7960999608039856\n",
      "epoch: 15\n",
      "train loss: 129.77178328856826 / dev loss: 37.65217661857605/ test loss:38.50804430246353 / dev acc:0.8115999698638916test acc:0.7964999675750732\n",
      "train loss: 129.21415056847036 / dev loss: 37.060867205262184/ test loss:37.93366456031799 / dev acc:0.8090999722480774test acc:0.8046000003814697\n",
      "train loss: 135.7345163449645 / dev loss: 37.88429753482342/ test loss:38.134562999010086 / dev acc:0.8050999641418457test acc:0.7999999523162842\n",
      "train loss: 136.2140307240188 / dev loss: 35.3166346848011/ test loss:36.28831885755062 / dev acc:0.8134999871253967test acc:0.7996999621391296\n",
      "train loss: 136.76033817976713 / dev loss: 35.283359333872795/ test loss:36.27982039749622 / dev acc:0.8118000030517578test acc:0.8011999726295471\n",
      "train loss: 139.75273233652115 / dev loss: 37.56232215464115/ test loss:38.49420239776373 / dev acc:0.8050000071525574test acc:0.7928999662399292\n",
      "max dev acc:0.8137999773025513/ max test acc: 0.8014000058174133\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'int' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7e95839dff7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-7e95839dff7c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/BIBPM_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'int' object to str implicitly"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from model.BIMPM import BIMPM\n",
    "from model.utils import SNLI, Quora\n",
    "from test import test\n",
    "\n",
    "\n",
    "def train(args, data):\n",
    "    model = (Siamese(args, data))\n",
    "    if args.gpu > -1:\n",
    "        model.cuda(args.gpu)\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_acc, max_test_acc = 0, 0\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', str(present_epoch + 1))\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        if args.data_type == 'SNLI':\n",
    "            s1, s2 = 'premise', 'hypothesis'\n",
    "        else:\n",
    "            s1, s2 = 'q1', 'q2'\n",
    "\n",
    "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
    "\n",
    "        # limit the lengths of input sentences up to max_sent_len\n",
    "        if args.max_sent_len >= 0:\n",
    "            if s1.size()[1] > args.max_sent_len:\n",
    "                s1 = s1[:, :args.max_sent_len]\n",
    "            if s2.size()[1] > args.max_sent_len:\n",
    "                s2 = s2[:, :args.max_sent_len]\n",
    "\n",
    "        kwargs = {'p': s1, 'h': s2}\n",
    "\n",
    "        if args.use_char_emb:\n",
    "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
    "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
    "\n",
    "            if args.gpu > -1:\n",
    "                char_p = char_p.cuda(args.gpu)\n",
    "                char_h = char_h.cuda(args.gpu)\n",
    "\n",
    "            kwargs['char_p'] = char_p\n",
    "            kwargs['char_h'] = char_h\n",
    "\n",
    "        pred = (model(**kwargs))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = criterion(pred, batch.label)\n",
    "        loss += batch_loss.data[0]\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        del pred\n",
    "        del batch_loss\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
    "            test_loss, test_acc = test(model, args, data)\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('acc/dev', dev_acc, c)\n",
    "            writer.add_scalar('loss/test', test_loss, c)\n",
    "            writer.add_scalar('acc/test', test_acc, c)\n",
    "\n",
    "            print('train loss: '+ str(loss) +' / dev loss: '+ str(dev_loss) + '/ test loss:' + str(test_loss) +\n",
    "                  ' / dev acc:' + str(dev_acc) + 'test acc:' + str(test_acc))\n",
    "\n",
    "            if dev_acc > max_dev_acc:\n",
    "                max_dev_acc = dev_acc\n",
    "                max_test_acc = test_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print('max dev acc:'+ str(max_dev_acc) + '/ max test acc: ' + str(max_test_acc))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', default=128, type=int)\n",
    "    parser.add_argument('--char-dim', default=20, type=int)\n",
    "    parser.add_argument('--char-hidden-size', default=50, type=int)\n",
    "    parser.add_argument('--data-type', default='Quora', help='available: SNLI or Quora')\n",
    "    parser.add_argument('--dropout', default=0.1, type=float)\n",
    "    parser.add_argument('--epoch', default=15, type=int)\n",
    "    parser.add_argument('--gpu', default=1, type=int)\n",
    "    parser.add_argument('--hidden-size', default=100, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.001, type=float)\n",
    "    parser.add_argument('--max-sent-len', default=-1, type=int,\n",
    "                        help='max length of input sentences model can accept, if -1, it accepts any length')\n",
    "    parser.add_argument('--num-perspective', default=20, type=int)\n",
    "    parser.add_argument('--print-freq', default=500, type=int)\n",
    "    parser.add_argument('--use-char-emb', default=False, action='store_true')\n",
    "    parser.add_argument('--word-dim', default=300, type=int)\n",
    "    parser.add_argument('--training', default=0, type=int)\n",
    "    args = parser.parse_args()\n",
    "    print(args.training)\n",
    "    if args.data_type == 'SNLI':\n",
    "        print('loading SNLI data...')\n",
    "        data = SNLI(args)\n",
    "    elif args.data_type == 'Quora':\n",
    "        print('loading Quora data...')\n",
    "        data = Quora(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
    "\n",
    "    setattr(args, 'char_vocab_size', len(data.char_vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.TEXT.vocab))\n",
    "    setattr(args, 'class_size', len(data.LABEL.vocab))\n",
    "    setattr(args, 'max_word_len', data.max_word_len)\n",
    "    setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), 'saved_models/BIBPM_'+args.data_type+'_'+args.model_time+'train'+args.training+'.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1, bidirectional=True)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "c0 = Variable(torch.randn(2, 3, 20))\n",
    "output, hn = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0262 -0.0300  0.1385  0.0517 -0.1187 -0.1581  0.0058 -0.0630 -0.0708\n",
       "   0.0431  0.0000 -0.1865  0.2206  0.1467 -0.2882 -0.0504 -0.3230 -0.0736\n",
       "   0.1322 -0.0603  0.1803  0.1033 -0.2160 -0.1069 -0.0303  0.1110  0.0982\n",
       " \n",
       " Columns 9 to 17 \n",
       "    0.1379 -0.0159 -0.0569  0.1671 -0.1168  0.1358 -0.0823  0.0798  0.0735\n",
       "   0.1638  0.1187 -0.0314 -0.1135 -0.1871  0.0551  0.1918 -0.1770 -0.1505\n",
       "  -0.0077  0.1058 -0.0803  0.1280 -0.0647 -0.0149  0.1065 -0.1494  0.1058\n",
       " \n",
       " Columns 18 to 19 \n",
       "    0.0836 -0.0075\n",
       "  -0.0716 -0.1395\n",
       "   0.1533  0.0331\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0218  0.1405  0.0528  0.0870  0.2355  0.0391  0.1179 -0.1470  0.1026\n",
       "  -0.1145 -0.0303 -0.1846 -0.0444 -0.2343  0.0695  0.0592 -0.0462  0.0413\n",
       "   0.1245  0.1497  0.0318 -0.0747  0.1270  0.1806 -0.0454 -0.1718  0.1462\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.0491  0.0722  0.0430  0.1969 -0.0574 -0.0376  0.1289 -0.0570  0.0927\n",
       "  -0.0145  0.0295 -0.0678  0.0056  0.1946  0.1184  0.1774 -0.1404 -0.0458\n",
       "  -0.1120 -0.0548 -0.0249  0.1601  0.1792  0.1098 -0.2562  0.0462 -0.0287\n",
       " \n",
       " Columns 18 to 19 \n",
       "   -0.1299  0.0748\n",
       "  -0.0381 -0.2646\n",
       "  -0.2266 -0.0379\n",
       " [torch.FloatTensor of size 2x3x20], Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0727 -0.0482  0.2978  0.0970 -0.2090 -0.3533  0.0137 -0.1320 -0.1972\n",
       "   0.0912  0.0000 -0.3299  0.5086  0.2623 -0.4963 -0.1592 -0.5498 -0.1705\n",
       "   0.4063 -0.1658  0.3195  0.2035 -0.4086 -0.2152 -0.0672  0.2179  0.2869\n",
       " \n",
       " Columns 9 to 17 \n",
       "    0.2457 -0.0312 -0.0978  0.2423 -0.2201  0.2292 -0.1269  0.3032  0.1604\n",
       "   0.3014  0.2765 -0.0835 -0.1674 -0.3090  0.1504  0.3951 -0.3920 -0.3827\n",
       "  -0.0155  0.3424 -0.2246  0.2083 -0.1162 -0.0298  0.2375 -0.3460  0.1651\n",
       " \n",
       " Columns 18 to 19 \n",
       "    0.1906 -0.0168\n",
       "  -0.1533 -0.2548\n",
       "   0.3252  0.1103\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.0631  0.2817  0.1168  0.2178  0.3444  0.0571  0.2109 -0.2296  0.2696\n",
       "  -0.1884 -0.0529 -0.3766 -0.0668 -0.3665  0.1372  0.1446 -0.0888  0.1378\n",
       "   0.2853  0.3587  0.0653 -0.1531  0.2440  0.3026 -0.0799 -0.3610  0.3513\n",
       " \n",
       " Columns 9 to 17 \n",
       "   -0.1209  0.1599  0.1192  0.3646 -0.0949 -0.0617  0.2403 -0.2089  0.1627\n",
       "  -0.0267  0.0678 -0.1334  0.0103  0.3997  0.2439  0.3168 -0.2974 -0.0745\n",
       "  -0.2430 -0.1239 -0.0604  0.3928  0.3393  0.2123 -0.5639  0.1221 -0.0612\n",
       " \n",
       " Columns 18 to 19 \n",
       "   -0.2121  0.1477\n",
       "  -0.0695 -0.3737\n",
       "  -0.4132 -0.0618\n",
       " [torch.FloatTensor of size 2x3x20])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
