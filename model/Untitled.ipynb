{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIMPM(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super(BIMPM, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.d = self.args.word_dim + int(self.args.use_char_emb) * self.args.char_hidden_size\n",
    "        self.l = self.args.num_perspective\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=0)\n",
    "\n",
    "        self.word_emb = nn.Embedding(args.word_vocab_size, args.word_dim)\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb.weight.data.copy_(data.TEXT.vocab.vectors)\n",
    "        # no fine-tuning for word vectors\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "\n",
    "        self.char_LSTM = nn.LSTM(\n",
    "            input_size=self.args.char_dim,\n",
    "            hidden_size=self.args.char_hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        self.context_LSTM = nn.LSTM(\n",
    "            input_size=self.d,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ----- Matching Layer -----\n",
    "        for i in range(1, 9):\n",
    "            setattr(self, f'mp_w{i}',\n",
    "                    nn.Parameter(torch.rand(self.l, self.args.hidden_size)))\n",
    "\n",
    "        # ----- Aggregation Layer -----\n",
    "        self.aggregation_LSTM = nn.LSTM(\n",
    "            input_size=self.l * 8,\n",
    "            hidden_size=self.args.hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        self.pred_fc1 = nn.Linear(self.args.hidden_size * 4, self.args.hidden_size * 2)\n",
    "        self.pred_fc2 = nn.Linear(self.args.hidden_size * 2, self.args.class_size)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # ----- Word Representation Layer -----\n",
    "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
    "        # zero vectors for padding\n",
    "        self.char_emb.weight.data[0].fill_(0)\n",
    "\n",
    "        # <unk> vectors is randomly initialized\n",
    "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
    "\n",
    "        nn.init.kaiming_normal(self.char_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.char_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.char_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_ih_l0_reverse, val=0)\n",
    "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0_reverse)\n",
    "        nn.init.constant(self.context_LSTM.bias_hh_l0_reverse, val=0)\n",
    "\n",
    "        # ----- Matching Layer -----\n",
    "        for i in range(1, 9):\n",
    "            w = getattr(self, f'mp_w{i}')\n",
    "            nn.init.kaiming_normal(w)\n",
    "\n",
    "        # ----- Aggregation Layer -----\n",
    "        nn.init.kaiming_normal(self.aggregation_LSTM.weight_ih_l0)\n",
    "        nn.init.constant(self.aggregation_LSTM.bias_ih_l0, val=0)\n",
    "        nn.init.orthogonal(self.aggregation_LSTM.weight_hh_l0)\n",
    "        nn.init.constant(self.aggregation_LSTM.bias_hh_l0, val=0)\n",
    "\n",
    "        nn.init.kaiming_normal(self.aggregation_LSTM.weight_ih_l0_reverse)\n",
    "        nn.init.constant(self.aggregation_LSTM.bias_ih_l0_reverse, val=0)\n",
    "        nn.init.orthogonal(self.aggregation_LSTM.weight_hh_l0_reverse)\n",
    "        nn.init.constant(self.aggregation_LSTM.bias_hh_l0_reverse, val=0)\n",
    "\n",
    "        # ----- Prediction Layer ----\n",
    "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
    "\n",
    "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
    "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
    "\n",
    "    def dropout(self, v):\n",
    "        return F.dropout(v, p=self.args.dropout, training=self.training)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # ----- Matching Layer -----\n",
    "        def mp_matching_func(v1, v2, w):\n",
    "            \"\"\"\n",
    "            :param v1: (batch, seq_len, hidden_size)\n",
    "            :param v2: (batch, seq_len, hidden_size) or (batch, hidden_size)\n",
    "            :param w: (l, hidden_size)\n",
    "            :return: (batch, l)\n",
    "            \"\"\"\n",
    "            seq_len = v1.size(1)\n",
    "\n",
    "            # Trick for large memory requirement\n",
    "            \"\"\"\n",
    "            if len(v2.size()) == 2:\n",
    "                v2 = torch.stack([v2] * seq_len, dim=1)\n",
    "            m = []\n",
    "            for i in range(self.l):\n",
    "                # v1: (batch, seq_len, hidden_size)\n",
    "                # v2: (batch, seq_len, hidden_size)\n",
    "                # w: (1, 1, hidden_size)\n",
    "                # -> (batch, seq_len)\n",
    "                m.append(F.cosine_similarity(w[i].view(1, 1, -1) * v1, w[i].view(1, 1, -1) * v2, dim=2))\n",
    "            # list of (batch, seq_len) -> (batch, seq_len, l)\n",
    "            m = torch.stack(m, dim=2)\n",
    "            \"\"\"\n",
    "\n",
    "            # (1, 1, hidden_size, l)\n",
    "            w = w.transpose(1, 0).unsqueeze(0).unsqueeze(0)\n",
    "            # (batch, seq_len, hidden_size, l)\n",
    "            v1 = w * torch.stack([v1] * self.l, dim=3)\n",
    "            if len(v2.size()) == 3:\n",
    "                v2 = w * torch.stack([v2] * self.l, dim=3)\n",
    "            else:\n",
    "                v2 = w * torch.stack([torch.stack([v2] * seq_len, dim=1)] * self.l, dim=3)\n",
    "\n",
    "            m = F.cosine_similarity(v1, v2, dim=2)\n",
    "\n",
    "            return m\n",
    "\n",
    "        def mp_matching_func_pairwise(v1, v2, w):\n",
    "            \"\"\"\n",
    "            :param v1: (batch, seq_len1, hidden_size)\n",
    "            :param v2: (batch, seq_len2, hidden_size)\n",
    "            :param w: (l, hidden_size)\n",
    "            :return: (batch, l, seq_len1, seq_len2)\n",
    "            \"\"\"\n",
    "\n",
    "            # Trick for large memory requirement\n",
    "            \"\"\"\n",
    "            m = []\n",
    "            for i in range(self.l):\n",
    "                # (1, 1, hidden_size)\n",
    "                w_i = w[i].view(1, 1, -1)\n",
    "                # (batch, seq_len1, hidden_size), (batch, seq_len2, hidden_size)\n",
    "                v1, v2 = w_i * v1, w_i * v2\n",
    "                # (batch, seq_len, hidden_size->1)\n",
    "                v1_norm = v1.norm(p=2, dim=2, keepdim=True)\n",
    "                v2_norm = v2.norm(p=2, dim=2, keepdim=True)\n",
    "                # (batch, seq_len1, seq_len2)\n",
    "                n = torch.matmul(v1, v2.permute(0, 2, 1))\n",
    "                d = v1_norm * v2_norm.permute(0, 2, 1)\n",
    "                m.append(div_with_small_value(n, d))\n",
    "            # list of (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, l)\n",
    "            m = torch.stack(m, dim=3)\n",
    "            \"\"\"\n",
    "\n",
    "            # (1, l, 1, hidden_size)\n",
    "            w = w.unsqueeze(0).unsqueeze(2)\n",
    "            # (batch, l, seq_len, hidden_size)\n",
    "            v1, v2 = w * torch.stack([v1] * self.l, dim=1), w * torch.stack([v2] * self.l, dim=1)\n",
    "            # (batch, l, seq_len, hidden_size->1)\n",
    "            v1_norm = v1.norm(p=2, dim=3, keepdim=True)\n",
    "            v2_norm = v2.norm(p=2, dim=3, keepdim=True)\n",
    "\n",
    "            # (batch, l, seq_len1, seq_len2)\n",
    "            n = torch.matmul(v1, v2.transpose(2, 3))\n",
    "            d = v1_norm * v2_norm.transpose(2, 3)\n",
    "\n",
    "            # (batch, seq_len1, seq_len2, l)\n",
    "            m = div_with_small_value(n, d).permute(0, 2, 3, 1)\n",
    "\n",
    "            return m\n",
    "\n",
    "        def attention(v1, v2):\n",
    "            \"\"\"\n",
    "            :param v1: (batch, seq_len1, hidden_size)\n",
    "            :param v2: (batch, seq_len2, hidden_size)\n",
    "            :return: (batch, seq_len1, seq_len2)\n",
    "            \"\"\"\n",
    "\n",
    "            # (batch, seq_len1, 1)\n",
    "            v1_norm = v1.norm(p=2, dim=2, keepdim=True)\n",
    "            # (batch, 1, seq_len2)\n",
    "            v2_norm = v2.norm(p=2, dim=2, keepdim=True).permute(0, 2, 1)\n",
    "\n",
    "            # (batch, seq_len1, seq_len2)\n",
    "            a = torch.bmm(v1, v2.permute(0, 2, 1))\n",
    "            d = v1_norm * v2_norm\n",
    "\n",
    "            return div_with_small_value(a, d)\n",
    "\n",
    "        def div_with_small_value(n, d, eps=1e-8):\n",
    "            # too small values are replaced by 1e-8 to prevent it from exploding.\n",
    "            d = d * (d > eps).float() + eps * (d <= eps).float()\n",
    "            return n / d\n",
    "\n",
    "        # ----- Word Representation Layer -----\n",
    "        # (batch, seq_len) -> (batch, seq_len, word_dim)\n",
    "\n",
    "        p = self.word_emb(kwargs['p'])\n",
    "        h = self.word_emb(kwargs['h'])\n",
    "\n",
    "        if self.args.use_char_emb:\n",
    "            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n",
    "            seq_len_p = kwargs['char_p'].size(1)\n",
    "            seq_len_h = kwargs['char_h'].size(1)\n",
    "\n",
    "            char_p = kwargs['char_p'].view(-1, self.args.max_word_len)\n",
    "            char_h = kwargs['char_h'].view(-1, self.args.max_word_len)\n",
    "\n",
    "            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n",
    "            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n",
    "            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n",
    "\n",
    "            # (batch, seq_len, char_hidden_size)\n",
    "            char_p = char_p.view(-1, seq_len_p, self.args.char_hidden_size)\n",
    "            char_h = char_h.view(-1, seq_len_h, self.args.char_hidden_size)\n",
    "\n",
    "            # (batch, seq_len, word_dim + char_hidden_size)\n",
    "            p = torch.cat([p, char_p], dim=-1)\n",
    "            h = torch.cat([h, char_h], dim=-1)\n",
    "\n",
    "        p = self.dropout(p)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # ----- Context Representation Layer -----\n",
    "        # (batch, seq_len, hidden_size * 2)\n",
    "        con_p, _ = self.context_LSTM(p)\n",
    "        con_h, _ = self.context_LSTM(h)\n",
    "\n",
    "        con_p = self.dropout(con_p)\n",
    "        con_h = self.dropout(con_h)\n",
    "\n",
    "        # (batch, seq_len, hidden_size)\n",
    "        con_p_fw, con_p_bw = torch.split(con_p, self.args.hidden_size, dim=-1)\n",
    "        con_h_fw, con_h_bw = torch.split(con_h, self.args.hidden_size, dim=-1)\n",
    "\n",
    "        # 1. Full-Matching\n",
    "\n",
    "        # (batch, seq_len, hidden_size), (batch, hidden_size)\n",
    "        # -> (batch, seq_len, l)\n",
    "        mv_p_full_fw = mp_matching_func(con_p_fw, con_h_fw[:, -1, :], self.mp_w1)\n",
    "        mv_p_full_bw = mp_matching_func(con_p_bw, con_h_bw[:, 0, :], self.mp_w2)\n",
    "        mv_h_full_fw = mp_matching_func(con_h_fw, con_p_fw[:, -1, :], self.mp_w1)\n",
    "        mv_h_full_bw = mp_matching_func(con_h_bw, con_p_bw[:, 0, :], self.mp_w2)\n",
    "\n",
    "        # 2. Maxpooling-Matching\n",
    "\n",
    "        # (batch, seq_len1, seq_len2, l)\n",
    "        mv_max_fw = mp_matching_func_pairwise(con_p_fw, con_h_fw, self.mp_w3)\n",
    "        mv_max_bw = mp_matching_func_pairwise(con_p_bw, con_h_bw, self.mp_w4)\n",
    "\n",
    "        # (batch, seq_len, l)\n",
    "        mv_p_max_fw, _ = mv_max_fw.max(dim=2)\n",
    "        mv_p_max_bw, _ = mv_max_bw.max(dim=2)\n",
    "        mv_h_max_fw, _ = mv_max_fw.max(dim=1)\n",
    "        mv_h_max_bw, _ = mv_max_bw.max(dim=1)\n",
    "\n",
    "        # 3. Attentive-Matching\n",
    "\n",
    "        # (batch, seq_len1, seq_len2)\n",
    "        att_fw = attention(con_p_fw, con_h_fw)\n",
    "        att_bw = attention(con_p_bw, con_h_bw)\n",
    "\n",
    "        # (batch, seq_len2, hidden_size) -> (batch, 1, seq_len2, hidden_size)\n",
    "        # (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, 1)\n",
    "        # -> (batch, seq_len1, seq_len2, hidden_size)\n",
    "        att_h_fw = con_h_fw.unsqueeze(1) * att_fw.unsqueeze(3)\n",
    "        att_h_bw = con_h_bw.unsqueeze(1) * att_bw.unsqueeze(3)\n",
    "        # (batch, seq_len1, hidden_size) -> (batch, seq_len1, 1, hidden_size)\n",
    "        # (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, 1)\n",
    "        # -> (batch, seq_len1, seq_len2, hidden_size)\n",
    "        att_p_fw = con_p_fw.unsqueeze(2) * att_fw.unsqueeze(3)\n",
    "        att_p_bw = con_p_bw.unsqueeze(2) * att_bw.unsqueeze(3)\n",
    "\n",
    "        # (batch, seq_len1, hidden_size) / (batch, seq_len1, 1) -> (batch, seq_len1, hidden_size)\n",
    "        att_mean_h_fw = div_with_small_value(att_h_fw.sum(dim=2), att_fw.sum(dim=2, keepdim=True))\n",
    "        att_mean_h_bw = div_with_small_value(att_h_bw.sum(dim=2), att_bw.sum(dim=2, keepdim=True))\n",
    "\n",
    "        # (batch, seq_len2, hidden_size) / (batch, seq_len2, 1) -> (batch, seq_len2, hidden_size)\n",
    "        att_mean_p_fw = div_with_small_value(att_p_fw.sum(dim=1), att_fw.sum(dim=1, keepdim=True).permute(0, 2, 1))\n",
    "        att_mean_p_bw = div_with_small_value(att_p_bw.sum(dim=1), att_bw.sum(dim=1, keepdim=True).permute(0, 2, 1))\n",
    "\n",
    "        # (batch, seq_len, l)\n",
    "        mv_p_att_mean_fw = mp_matching_func(con_p_fw, att_mean_h_fw, self.mp_w5)\n",
    "        mv_p_att_mean_bw = mp_matching_func(con_p_bw, att_mean_h_bw, self.mp_w6)\n",
    "        mv_h_att_mean_fw = mp_matching_func(con_h_fw, att_mean_p_fw, self.mp_w5)\n",
    "        mv_h_att_mean_bw = mp_matching_func(con_h_bw, att_mean_p_bw, self.mp_w6)\n",
    "\n",
    "        # 4. Max-Attentive-Matching\n",
    "\n",
    "        # (batch, seq_len1, hidden_size)\n",
    "        att_max_h_fw, _ = att_h_fw.max(dim=2)\n",
    "        att_max_h_bw, _ = att_h_bw.max(dim=2)\n",
    "        # (batch, seq_len2, hidden_size)\n",
    "        att_max_p_fw, _ = att_p_fw.max(dim=1)\n",
    "        att_max_p_bw, _ = att_p_bw.max(dim=1)\n",
    "\n",
    "        # (batch, seq_len, l)\n",
    "        mv_p_att_max_fw = mp_matching_func(con_p_fw, att_max_h_fw, self.mp_w7)\n",
    "        mv_p_att_max_bw = mp_matching_func(con_p_bw, att_max_h_bw, self.mp_w8)\n",
    "        mv_h_att_max_fw = mp_matching_func(con_h_fw, att_max_p_fw, self.mp_w7)\n",
    "        mv_h_att_max_bw = mp_matching_func(con_h_bw, att_max_p_bw, self.mp_w8)\n",
    "\n",
    "        # (batch, seq_len, l * 8)\n",
    "        mv_p = torch.cat(\n",
    "            [mv_p_full_fw, mv_p_max_fw, mv_p_att_mean_fw, mv_p_att_max_fw,\n",
    "             mv_p_full_bw, mv_p_max_bw, mv_p_att_mean_bw, mv_p_att_max_bw], dim=2)\n",
    "        mv_h = torch.cat(\n",
    "            [mv_h_full_fw, mv_h_max_fw, mv_h_att_mean_fw, mv_h_att_max_fw,\n",
    "             mv_h_full_bw, mv_h_max_bw, mv_h_att_mean_bw, mv_h_att_max_bw], dim=2)\n",
    "\n",
    "        mv_p = self.dropout(mv_p)\n",
    "        mv_h = self.dropout(mv_h)\n",
    "\n",
    "        # ----- Aggregation Layer -----\n",
    "        # (batch, seq_len, l * 8) -> (2, batch, hidden_size)\n",
    "        _, (agg_p_last, _) = self.aggregation_LSTM(mv_p)\n",
    "        _, (agg_h_last, _) = self.aggregation_LSTM(mv_h)\n",
    "\n",
    "        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n",
    "        x = torch.cat(\n",
    "            [agg_p_last.permute(1, 0, 2).contiguous().view(-1, self.args.hidden_size * 2),\n",
    "             agg_h_last.permute(1, 0, 2).contiguous().view(-1, self.args.hidden_size * 2)], dim=1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # ----- Prediction Layer -----\n",
    "        x = F.tanh(self.pred_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred_fc2(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
